{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CS-344 Artificial Intelligence - Homework 2 - Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Build a spam filter based on Paul Graham’s \"A Plan for Spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing spam filter algorithm!\n",
      "I like Spam! - Delicious!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "occurrences of each word in spam dictionary: [Counter({'I': 2, 'am': 2, 'spam': 2}), Counter({'I': 1, 'do': 1, 'not': 1, 'like': 1, 'that': 1, 'spamiam': 1})]\n",
      "occurrences of each word in non-spam dictionary: [Counter({'do': 1, 'i': 1, 'like': 1, 'green': 1, 'eggs': 1, 'and': 1, 'ham': 1}), Counter({'i': 1, 'do': 1})]\n",
      "\n",
      "lower-case only word spam word occurrences: {'i': 3, 'am': 2, 'spam': 2, 'do': 1, 'not': 1, 'like': 1, 'that': 1, 'spamiam': 1}\n",
      "lower-case only word non-spam word occurrences: {'do': 2, 'i': 2, 'like': 1, 'green': 1, 'eggs': 1, 'and': 1, 'ham': 1}\n",
      "words spam probabilities: {'do': 0.99, 'i': 0.99, 'like': 0.99, 'green': 0.01, 'eggs': 0.01, 'and': 0.01, 'ham': 0.01}\n",
      "\n",
      "word spam probability values only, keys removed: [0.01, 0.01, 0.01, 0.01, 0.99, 0.99, 0.99]\n",
      "product of individual values: 9.702990000000001e-09\n",
      "word spam complement probability values: [0.99, 0.99, 0.99, 0.99, 0.010000000000000009, 0.010000000000000009, 0.010000000000000009]\n",
      "product of complement values: 9.605960100000026e-07\n",
      "final probability message is spam: 0.009999999999999974\n",
      "Not spam.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Course: CS 344 - Artificial Intelligence\n",
    "Instructor: Professor VanderLinden\n",
    "Name: Joseph Jinn\n",
    "Date: 3-1-19\n",
    "\n",
    "Homework 2 - Uncertainty\n",
    "Spam filter - LISP to Python algorithm conversion (from Paul Graham's A Plan for Spam)\n",
    "\n",
    "Notes:\n",
    "\n",
    "Include in your solution, as one test case, the probability tables for the words in the following hard-coded\n",
    "SPAM/HAM corpus (and only this corpus) using a minimum count threshold of 1 (rather than the 5 used in the algorithm)\n",
    "\n",
    "Oh, god, functional languages...\n",
    "\n",
    "Resources Used:\n",
    "\n",
    "https://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item\n",
    "(I used this to figure out how to count the number of occurrences of each token.)\n",
    "\n",
    "https://stackoverflow.com/questions/11068986/how-to-convert-counter-object-to-dict\n",
    "(convert a \"Counter\" to a dict\n",
    "\n",
    "https://stackoverflow.com/questions/3294889/iterating-over-dictionaries-using-for-loops\n",
    "(how to iterate through all items in a dictionary)\n",
    "\n",
    "https://stackoverflow.com/questions/3845362/how-can-i-check-if-a-key-exists-in-a-dictionary\n",
    "(how to check if a key exists or doesn't exist in a dictionary)\n",
    "\n",
    "https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/\n",
    "(sort a dictionary by its values)\n",
    "\n",
    "https://www.tutorialspoint.com/How-to-convert-Python-dictionary-keys-values-to-lowercase\n",
    "(convert dict keys to lower-case)\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Spam corpus contains spam e-mails.\n",
    "spam_corpus = [[\"I\", \"am\", \"spam\", \"spam\", \"I\", \"am\"], [\"I\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "# Ham corpus contains non-spam e-mails.\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "\"\"\"\n",
    "The especially observant will notice that while I consider each corpus to be a single long stream of text for purposes \n",
    "of counting occurrences, I use the number of emails in each, rather than their combined length, \n",
    "as the divisor in calculating spam probabilities. This adds another slight bias to protect against false positives.\n",
    "\"\"\"\n",
    "# Number of spam and non-spam e-mails messages (not words).\n",
    "number_bad_message = len(spam_corpus)\n",
    "number_good_messages = len(spam_corpus)\n",
    "\n",
    "# Threshold value for the spam filter algorithm.\n",
    "algorithm_threshold_value = 1\n",
    "spam_message_threshold_value = 0.9\n",
    "interesting_tokens_threshold_value = 15\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "class SpamFilter:\n",
    "    \"\"\"\n",
    "    The SpamFilter class accepts a corpus and analyzes it to determine what words are spam and what words are\n",
    "    legitimate.\n",
    "\n",
    "    # TODO - refactor spam filter to be class-based rather than just individual floating functions.\n",
    "    Note: Placeholder for now.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus, threshold):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        :param corpus: list of words to analyze.\n",
    "        :param threshold:  threshold value for the algorithm.\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "def word_occurrences(corpus):\n",
    "    \"\"\"\n",
    "    Counts the number of times each word occurs in the message.\n",
    "\n",
    "    :param corpus: list of words to count occurrences for.\n",
    "    :return:  dictionary of each word and their occurrences.\n",
    "\n",
    "    Here's a sketch of how I do statistical filtering.\n",
    "    I start with one corpus of spam and one of nonspam mail.\n",
    "    At the moment each one has about 4000 messages in it.\n",
    "    I scan the entire text, including headers and embedded html and javascript, of each message in each corpus.\n",
    "    I currently consider alphanumeric characters, dashes, apostrophes, and dollar signs to be part of tokens,\n",
    "    and everything else to be a token separator. (There is probably room for improvement here.)\n",
    "    I ignore tokens that are all digits, and I also ignore html comments,\n",
    "    not even considering them as token separators.\n",
    "\n",
    "    I count the number of times each token (ignoring case, currently) occurs in each corpus.\n",
    "    At this stage I end up with two large hash tables, one for each corpus, mapping tokens to number of occurrences.\n",
    "    \"\"\"\n",
    "    occur_array = []\n",
    "\n",
    "    for e in corpus:\n",
    "        occur = Counter(e)\n",
    "        occur_array.append(occur)\n",
    "\n",
    "    return occur_array\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def individual_word_spam_chance(spam_words_dict, non_spam_words_dict, threshold):\n",
    "    \"\"\"\n",
    "    TODO - ensure this is how the algorithm is meant to function - ask Professor VanderLinden.\n",
    "    Determines the spam'liness of each individual word in the message.\n",
    "\n",
    "    :param spam_words_dict: dictionary containing spam words.\n",
    "    :param non_spam_words_dict: dictionary containing non-spam words.\n",
    "    :param threshold: threshold value for statistical algorithm.\n",
    "    :return:  spam'liness value of each individual word as a dictionary.\n",
    "\n",
    "        Next I create a third hash table, this time mapping each token to the probability that an email containing\n",
    "    it is a spam, which I calculate as follows [1]:\n",
    "\n",
    "    (let ((g (* 2 (or (gethash word good) 0)))\n",
    "          (b (or (gethash word bad) 0)))\n",
    "       (unless (< (+ g b) 5)\n",
    "         (max .01\n",
    "              (min .99 (float (/ (min 1 (/ b nbad))\n",
    "                                 (+ (min 1 (/ g ngood))\n",
    "                                    (min 1 (/ b nbad)))))))))\n",
    "\n",
    "    where word is the token whose probability we're calculating,\n",
    "    good and bad are the hash tables I created in the first step,\n",
    "    and ngood and nbad are the number of nonspam and spam messages respectively.\n",
    "\n",
    "    FIXME - is this the right way of calculating the probabilities? (refer to passage below from \"A Plan for Spam\")\n",
    "\n",
    "    One question that arises in practice is what probability to assign to a word you've never seen, i.e.\n",
    "    one that doesn't occur in the hash table of word probabilities.\n",
    "    I've found, again by trial and error, that .4 is a good number to use.\n",
    "    If you've never seen a word before, it is probably fairly innocent; spam words tend to be all too familiar.\n",
    "    \"\"\"\n",
    "\n",
    "    words_spam_chance = {}\n",
    "\n",
    "    # Iterate through all non-spam words.\n",
    "    for good_key, good_value in non_spam_words_dict.items():\n",
    "\n",
    "        # If no associated value for each word set to 0, otherwise multiply value by 2.\n",
    "        if good_value is None:\n",
    "            good_occurrences = 0\n",
    "        else:\n",
    "            good_occurrences = 2 * good_value\n",
    "\n",
    "        # Determine if non-spam word has match in spam word dictionary.\n",
    "        # If no, set to 0.  If yes, set to value of non-spam word.\n",
    "        if good_key not in spam_words_dict:\n",
    "            bad_occurrences = 0\n",
    "        else:\n",
    "            bad_occurrences = spam_words_dict[good_key]\n",
    "\n",
    "        # Statistical algorithm to calculate the associated probability for each word.\n",
    "        if good_occurrences + bad_occurrences > threshold:\n",
    "            probability = max(0.01, min(0.99, min(1.0, bad_occurrences / number_bad_message) /\n",
    "                                        min(1.0, good_occurrences / number_good_messages) +\n",
    "                                        min(1.0, bad_occurrences / number_bad_message)))\n",
    "        else:\n",
    "            probability = 0\n",
    "\n",
    "        # Store to dictionary each word and their associated probability.\n",
    "        words_spam_chance[good_key] = probability\n",
    "\n",
    "    # Return our dictionary of stored words spam probabilities.\n",
    "    return words_spam_chance\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def message_spam_chance(word_probabilities_dict):\n",
    "    \"\"\"\n",
    "    Determines the final probability of being spam.\n",
    "\n",
    "    :param word_probabilities_dict: individual probabilities for each word.\n",
    "    :return: probability the message is spam.\n",
    "\n",
    "    If probs is a list of the fifteen individual probabilities, you calculate the combined probability thus:\n",
    "\n",
    "    (let ((prod (apply #'* probs)))\n",
    "      (/ prod (+ prod (apply #'* (mapcar #'(lambda (x)\n",
    "                                             (- 1 x))\n",
    "                                         probs)))))\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove the keys from the dictionary and store only the associated values.\n",
    "    word_probability_values = sorted(word_probabilities_dict.values())\n",
    "    print(\"\\nword spam probability values only, keys removed: \" + str(word_probability_values))\n",
    "\n",
    "    # Calculate the product of all individual word probabilities.\n",
    "    product_of_probabilities = 1.0\n",
    "    for each_probability in word_probability_values:\n",
    "        product_of_probabilities *= each_probability\n",
    "\n",
    "    print(\"product of individual values: \" + str(product_of_probabilities))\n",
    "\n",
    "    # Determine the complement of all individual word probabilities.\n",
    "    word_probability_complement_values = []\n",
    "    for each_probability in word_probability_values:\n",
    "        complement = 1.00 - each_probability\n",
    "        word_probability_complement_values.append(complement)\n",
    "\n",
    "    print(\"word spam complement probability values: \" + str(word_probability_complement_values))\n",
    "\n",
    "    # Calculate the product of all complement probabilities.\n",
    "    product_of_complement_probabilities = 1.0\n",
    "    for each_complement_probability in word_probability_complement_values:\n",
    "        product_of_complement_probabilities *= each_complement_probability\n",
    "\n",
    "    print(\"product of complement values: \" + str(product_of_complement_probabilities))\n",
    "\n",
    "    spam_message_probability = product_of_probabilities / \\\n",
    "                               (product_of_probabilities + product_of_complement_probabilities)\n",
    "\n",
    "    print(\"final probability message is spam: \" + str(spam_message_probability))\n",
    "\n",
    "    return spam_message_probability\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "def find_interesting_tokens(word_spam_chance_dict):\n",
    "    \"\"\"\n",
    "    Prunes dictionary containing the words in the message to the most interesting 15 tokens based\n",
    "    on the size of their deviation from the \"Neutral\" value of 0.5\n",
    "\n",
    "    :param word_spam_chance_dict: dictionary containing the spam probabilities of each word.\n",
    "    :return: the 15 most interesting words and their associated spam probabilities.\n",
    "\n",
    "    FIXME - set cut-off value to 15 after we are finished testing (interesting_tokens_threshold_value).\n",
    "\n",
    "    When new mail arrives, it is scanned into tokens, and the most interesting fifteen tokens,\n",
    "    where interesting is measured by how far their spam probability is from a neutral .5,\n",
    "    are used to calculate the probability that the mail is spam.\n",
    "    \"\"\"\n",
    "    # If more than 15 tokens, prune to the most \"interesting\" 15.\n",
    "    if len(word_spam_chance_dict) > interesting_tokens_threshold_value:\n",
    "\n",
    "        # Determine the 15 tokens with the largest deviation from neutral 0.5.\n",
    "        normalized_word_spam_chance = {}\n",
    "        for key, value in word_spam_chance_dict.items():\n",
    "            normalized_word_spam_chance[key] = abs(0.5 - value)\n",
    "        print(\"\\nnormalized word spam chances: \" + str(normalized_word_spam_chance))\n",
    "\n",
    "        # Sort dictionary so that largest deviations are at the front.\n",
    "        sorted_dict = sorted(normalized_word_spam_chance.items())\n",
    "        print(\"\\nmy sorted dict with normalized values: \" + str(sorted_dict))\n",
    "\n",
    "        # Slice dictionary so only first 15 key-value pairs are left.\n",
    "        slice_dict = islice(sorted_dict, interesting_tokens_threshold_value)\n",
    "\n",
    "        # Convert to dictionary as islice returns an iterator.\n",
    "        first15 = {}\n",
    "        for each in slice_dict:\n",
    "            first15[each[0]] = each[1]\n",
    "        print(\"\\nfirst 15 tokens with normalized keys and values: \" + str(first15))\n",
    "\n",
    "        # Un-normalize and return to original values by assigning original values.\n",
    "        first15_unnormalized = {}\n",
    "        for key, value in first15.items():\n",
    "            first15_unnormalized[key] = word_spam_chance_dict[key]\n",
    "        print(\"first 15 tokens un-normalized keys and values: \" + str(first15_unnormalized))\n",
    "    else:\n",
    "        return word_spam_chance_dict\n",
    "\n",
    "    return first15_unnormalized\n",
    "\n",
    "\n",
    "def convert_to_lowercase_and_combine_dict(spam_occurrences, nonspam_occurrences):\n",
    "    \"\"\"\n",
    "    Convert all keys to lower-case as words should not be case-sensitive.\n",
    "    Also, collapse all messages and their individual words into one combined dictionary containing\n",
    "    all unique component words and their occurrences.\n",
    "\n",
    "    :param spam_occurrences: contains all spam words and their occurrences as multiple dictionaries.\n",
    "    :param nonspam_occurrences: contains all non-spam words and their occurrences as multiple dictionaries.\n",
    "    :return:  two dictionaries, one for spam and one for non-spam, containing their lower-case words and\n",
    "    associated occurrences.\n",
    "    \"\"\"\n",
    "    lowercase_only_spam_word_occurrences = {}\n",
    "    for each_dictionary in spam_occurrences:\n",
    "        for key, value in each_dictionary.items():\n",
    "            if key.lower() not in lowercase_only_spam_word_occurrences:\n",
    "                lowercase_only_spam_word_occurrences[key.lower()] = value\n",
    "            else:\n",
    "                lowercase_only_spam_word_occurrences[key.lower()] += value\n",
    "    print(\"\\nlower-case only word spam word occurrences: \" + str(lowercase_only_spam_word_occurrences))\n",
    "\n",
    "    lowercase_only_nonspam_word_occurrences = {}\n",
    "    for each_dictionary in nonspam_occurrences:\n",
    "        for key, value in each_dictionary.items():\n",
    "            if key.lower() not in lowercase_only_nonspam_word_occurrences:\n",
    "                lowercase_only_nonspam_word_occurrences[key.lower()] = value\n",
    "            else:\n",
    "                lowercase_only_nonspam_word_occurrences[key.lower()] += value\n",
    "    print(\"lower-case only word non-spam word occurrences: \" + str(lowercase_only_nonspam_word_occurrences))\n",
    "\n",
    "    all_occurrences = [lowercase_only_spam_word_occurrences, lowercase_only_nonspam_word_occurrences]\n",
    "    return all_occurrences\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Pithy Introduction.\n",
    "    Executes the program.\n",
    "    \"\"\"\n",
    "    print(\"\\nExecuting spam filter algorithm!\")\n",
    "    print(\"I like Spam! - Delicious!\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Get occurrences of each word in the list of words - returned as dictionary.\n",
    "    spamWordOccurrencesDict = word_occurrences(spam_corpus)\n",
    "    nonSpamWordOccurrencesDict = word_occurrences(ham_corpus)\n",
    "    print(\"\\noccurrences of each word in spam dictionary: \" + str(spamWordOccurrencesDict))\n",
    "    print(\"occurrences of each word in non-spam dictionary: \" + str(nonSpamWordOccurrencesDict))\n",
    "\n",
    "    # Convert words to lower-case, sum occurrences of unique words, and return as list containing 2 dictionaries -\n",
    "    # one for spam and another for non-spam.\n",
    "    combined_occurrences = convert_to_lowercase_and_combine_dict(spamWordOccurrencesDict, nonSpamWordOccurrencesDict)\n",
    "\n",
    "    # Determine probability that each word in the message is spam - returned as dictionary.\n",
    "    word_spam_chance = individual_word_spam_chance(combined_occurrences[0],\n",
    "                                                   combined_occurrences[1], algorithm_threshold_value)\n",
    "    print(\"words spam probabilities: \" + str(word_spam_chance))\n",
    "\n",
    "    # Obtain the 15 most interesting tokens in the message.\n",
    "    interesting_words_only = find_interesting_tokens(word_spam_chance)\n",
    "\n",
    "    # Obtain the spam message probability value.\n",
    "    result = message_spam_chance(interesting_words_only)\n",
    "\n",
    "    # Compare spam message probability value against threshold spam probability value.\n",
    "    if result >= spam_message_threshold_value:\n",
    "        print(\"Spam!!!!\")\n",
    "    else:\n",
    "        print(\"Not spam.\")\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Graham argues that this is a Bayesian approach to SPAM. What makes it Bayesian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike feature-recognizing filters which assigns an arbitrary spam \"score\" to an e-mail, the Bayaesian approach assigns actual probabilities.  Therefore, you know what you are measuring and there is little ambiguity about how the evidence should be combined to calculate the chances of the e-mail being spam.\n",
    "\n",
    "The Bayesian approach considers all evidence, the good evidence decreasing the likelihood of the message being spam and the bad evidence increasing the likelihood of the message being spam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Bayesian networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian network used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cs.calvin.edu/courses/cs/344/kvlinden/05bayesnets/images/figure14_12.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://cs.calvin.edu/courses/cs/344/kvlinden/05bayesnets/images/figure14_12.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5a97eb4939a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m '''\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprobability\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBayesNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menumeration_ask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melimination_ask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgibbs_ask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrejection_sampling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlikelihood_weighting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Utility variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'probability'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This module implements the Bayesian network shown in the text, Figure 14.2.\n",
    "It's taken from the AIMA Python code.\n",
    "\n",
    "@author: kvlinden\n",
    "@version Jan 2, 2013\n",
    "'''\n",
    "\n",
    "from probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask, rejection_sampling, likelihood_weighting\n",
    "\n",
    "# Utility variables\n",
    "T, F = True, False\n",
    "\n",
    "# From AIMA code (probability.py) - Fig. 14.2 - burglary example\n",
    "cloudy = BayesNet([\n",
    "    ('Cloudy', '', 0.50),\n",
    "    ('Rain', 'Cloudy', {T: 0.80, F: 0.20}),\n",
    "    ('Sprinkler', 'Cloudy', {T: 0.10, F: 0.50}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T): 0.99, (T, F): 0.90, (F, T): 0.90, (F, F): 0.00})\n",
    "    ])\n",
    "\n",
    "# Compute P(Cloudy)\n",
    "print(\"\\nP(Cloudy)\")\n",
    "print(enumeration_ask('Cloudy', dict(), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('Cloudy', dict(), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('Cloudy', dict(), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('Cloudy', dict(), cloudy).show_approx())\n",
    "print(likelihood_weighting('Cloudy', dict(), cloudy).show_approx())\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Compute P(Sprinkler | cloudy)\n",
    "print(\"\\nP(Sprinkler | cloudy)\")\n",
    "print(enumeration_ask('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "print(likelihood_weighting('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Compute P(Cloudy| the sprinkler is running and it’s not raining)\n",
    "print(\"\\nP(Cloudy| the sprinkler is running and it’s not raining)\")\n",
    "print(enumeration_ask('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "print(likelihood_weighting('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Compute P(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\n",
    "print(\"\\nP(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\")\n",
    "print(enumeration_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "print(likelihood_weighting('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Compute P(Cloudy | the grass is not wet)\n",
    "print(\"\\nP(Cloudy | the grass is not wet)\")\n",
    "print(enumeration_ask('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "print(likelihood_weighting('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "\n",
    "# TODO - fix module not found error in jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand calculated probabilities for Bayesian network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Refer to screen captures in Homework 2 directory and/or turned in hard-copy of hand calculations. (will enter hand-calculations using latex syntax if/when I have time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center; transform: rotate(90deg); margin: 0 10px 10px 0\" src=\"bayesian_network_hand_calc_page1.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center; transform: rotate(90deg); margin: 0 10px 10px 0\" src=\"bayesian_network_hand_calc_page2.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center; transform: rotate(90deg); margin: 0 10px 10px 0\" src=\"bayesian_network_hand_calc_page3.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: TODO - Images are cut-off for some reason.  Fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example *causal* computation** \n",
    "\n",
    "This computes the probability that John calls given that there as been a burglary but no earthquake.\n",
    "\n",
    "$\\begin{aligned}\n",
    "    \\textbf{P}(J | b \\land \\neg e)\n",
    "        &= \\alpha \\sum_a{\\textbf{P}(J, a, b, \\neg e)} \\\\\n",
    "        &= \\alpha \\sum_a{\\textbf{P}(J|a) \\cdot P(a|b,\\neg e) \\cdot P(b) \\cdot P(\\neg e)} \\\\\n",
    "        &= \\alpha \\cdot P(b) \\cdot P(\\neg e) \\cdot \\sum_a{P(J|a) \\cdot P(a|b,-e)} \\\\\n",
    "        &= \\alpha \\cdot 0.001 \\cdot 0.998 \\cdot \\langle(0.9 \\cdot 0.94 + 0.05 \\cdot 0.06), (0.1 \\cdot 0.94 + 0.95 \\cdot 0.06)\\rangle \\\\\n",
    "        &= \\alpha \\langle0.000847, 0.000150698\\rangle \\\\\n",
    "        &= \\langle0.85, 0.15\\rangle\n",
    "    \\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example *diagnostic* computation** \n",
    "\n",
    "This computes the probability that there as been a burglary given that the alarm has rung.\n",
    "\n",
    "$\\begin{aligned}\n",
    "                \\textbf{P}(B | a)\n",
    "                    &= \\alpha \\sum_e{\\textbf{P}(B, e, a)} \\\\\n",
    "                    &= \\alpha \\sum_e{\\textbf{P}(B) * P(e) * P(a | B, e)} \\\\\n",
    "                    &= \\alpha * \\langle (0.001 * 0.002 * 0.95 + 0.001 * 0.998 * 0.94), (0.999 * 0.002 * 0.29 + 0.999 * 0.998 * 0.001)\\rangle \\\\\n",
    "                    &= \\alpha * \\langle0.00094002, 0.001576422\\rangle \\\\\n",
    "                    &= \\langle0.374, 0.626\\rangle\n",
    "                \\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Compute the number of independent values in the full joint probability distribution for this domain. Assume that no conditional independence relations are known to hold between these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Ensure my answer is correct.\n",
    "\n",
    "There are n=4 binary random variables, therefore the number of independent parameters is 2^(n=4)-1.  This results in 2^4-1 = 15 independent parameters in total.\n",
    "\n",
    "Resources Used:\n",
    "\n",
    "http://www.cs.brandeis.edu/~cs134/K_F_Ch3.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the number of independent values in the Bayesian network for this domain. Assume the conditional independence relations implied by the Bayes network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Ensure my answer is correct.\n",
    "\n",
    "There are n=4 binary random variables, therefore the number of independent parameters is 2*(n=4)-1.  This results in 2*4-1 = 7 independent parameters in total.\n",
    "\n",
    "Resources Used:\n",
    "\n",
    "http://www.cs.brandeis.edu/~cs134/K_F_Ch3.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
