{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CS-344 Artificial Intelligence - Homework 2 - Uncertainty\n",
    "\n",
    "by: Joseph Jinn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Build a spam filter based on Paul Graham’s \"A Plan for Spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "\"\"\"\n",
    "ALL WORDS SPAM PROBABILITIES (should make sense): {'i': 0.5, 'am': 0.99, 'spam': 0.99, 'do': 0.5, 'not': 0.0, 'like': 0.5, 'that': 0.0, 'spamiam': 0.0, 'green': 0.01, 'eggs': 0.01, 'and': 0.01, 'ham': 0.01}\n",
    "\"\"\"\n",
    "\n",
    "The above example is the part of the program output that contains the calculated word spam probability for each word in our dataset obtained from the corpus'es!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing spam filter algorithm!\n",
      "I like Spam! - Delicious!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "occurrences of each word in spam dictionary: [Counter({'I': 2, 'am': 2, 'spam': 2}), Counter({'I': 1, 'do': 1, 'not': 1, 'like': 1, 'that': 1, 'spamiam': 1})]\n",
      "occurrences of each word in non-spam dictionary: [Counter({'do': 1, 'i': 1, 'like': 1, 'green': 1, 'eggs': 1, 'and': 1, 'ham': 1}), Counter({'i': 1, 'do': 1})]\n",
      "\n",
      "lower-case only word spam word occurrences: {'i': 3, 'am': 2, 'spam': 2, 'do': 1, 'not': 1, 'like': 1, 'that': 1, 'spamiam': 1}\n",
      "lower-case only word non-spam word occurrences: {'do': 2, 'i': 2, 'like': 1, 'green': 1, 'eggs': 1, 'and': 1, 'ham': 1}\n",
      "\n",
      "lower-case only spam word occurrences as part of one combined dictionary: {'i': 3, 'am': 2, 'spam': 2, 'do': 1, 'not': 1, 'like': 1, 'that': 1, 'spamiam': 1}\n",
      "lower-case only spam and non-spam word occurrences as part of one combined dictionary: {'i': 5, 'am': 2, 'spam': 2, 'do': 3, 'not': 1, 'like': 2, 'that': 1, 'spamiam': 1, 'green': 1, 'eggs': 1, 'and': 1, 'ham': 1}\n",
      "ALL WORDS SPAM PROBABILITIES (should make sense): {'i': 0.5, 'am': 0.99, 'spam': 0.99, 'do': 0.5, 'not': 0.0, 'like': 0.5, 'that': 0.0, 'spamiam': 0.0, 'green': 0.01, 'eggs': 0.01, 'and': 0.01, 'ham': 0.01}\n",
      "\n",
      "contents of test corpus word probability dictionary: {'i': 0.5, 'like': 0.5, 'starcraft': 0.4, '2': 0.4}\n",
      "\n",
      "normalized word spam chances: {'i': 0.010000000000000009, 'like': 0.010000000000000009, 'starcraft': 0.09999999999999998, '2': 0.09999999999999998}\n",
      "\n",
      "sorted normalized word spam chances: [('2', 0.09999999999999998), ('i', 0.010000000000000009), ('like', 0.010000000000000009), ('starcraft', 0.09999999999999998)]\n",
      "\n",
      "first 15 tokens with normalized keys and values: {'2': 0.09999999999999998, 'i': 0.010000000000000009, 'like': 0.010000000000000009, 'starcraft': 0.09999999999999998}\n",
      "first 15 tokens un-normalized keys and values: {'2': 0.4, 'i': 0.5, 'like': 0.5, 'starcraft': 0.4}\n",
      "\n",
      "word spam probability values only, keys removed: [0.4, 0.4, 0.5, 0.5]\n",
      "product of individual values: 0.04000000000000001\n",
      "word spam complement probability values: [0.6, 0.6, 0.5, 0.5]\n",
      "product of complement values: 0.09\n",
      "final probability message is spam: 0.30769230769230776\n",
      "\n",
      "Threshold value above which e-mail message is considered spam: 0.9\n",
      "Not spam.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Course: CS 344 - Artificial Intelligence\n",
    "Instructor: Professor VanderLinden\n",
    "Name: Joseph Jinn\n",
    "Date: 3-1-19\n",
    "\n",
    "Homework 2 - Uncertainty\n",
    "Spam filter - LISP to Python algorithm conversion (from Paul Graham's A Plan for Spam)\n",
    "\n",
    "Notes:\n",
    "\n",
    "Include in your solution, as one test case, the probability tables for the words in the following hard-coded\n",
    "SPAM/HAM corpus (and only this corpus) using a minimum count threshold of 1 (rather than the 5 used in the algorithm)\n",
    "\n",
    "Oh, god, functional languages...\n",
    "\n",
    "############################################\n",
    "\n",
    "Resources Used:\n",
    "\n",
    "https://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item\n",
    "(I used this to figure out how to count the number of occurrences of each token.)\n",
    "\n",
    "https://stackoverflow.com/questions/11068986/how-to-convert-counter-object-to-dict\n",
    "(convert a \"Counter\" to a dict\n",
    "\n",
    "https://stackoverflow.com/questions/3294889/iterating-over-dictionaries-using-for-loops\n",
    "(how to iterate through all items in a dictionary)\n",
    "\n",
    "https://stackoverflow.com/questions/3845362/how-can-i-check-if-a-key-exists-in-a-dictionary\n",
    "(how to check if a key exists or doesn't exist in a dictionary)\n",
    "\n",
    "https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/\n",
    "(sort a dictionary by its values)\n",
    "\n",
    "https://www.tutorialspoint.com/How-to-convert-Python-dictionary-keys-values-to-lowercase\n",
    "(convert dict keys to lower-case)\n",
    "\"\"\"\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Spam corpus contains spam e-mails.\n",
    "spam_corpus = [[\"I\", \"am\", \"spam\", \"spam\", \"I\", \"am\"], [\"I\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "# Ham corpus contains non-spam e-mails.\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "# Test corpus\n",
    "test_corpus = [[\"I\", \"like\", \"Starcraft\", \"2\"], [\"Protoss\", \"probe\", \"spam\", \"is\", \"annoying\"],\n",
    "               [\"You\", \"must\", \"construct\", \"additional\", \"pylons\"]]\n",
    "\n",
    "\"\"\"\n",
    "The especially observant will notice that while I consider each corpus to be a single long stream of text for purposes \n",
    "of counting occurrences, I use the number of emails in each, rather than their combined length, \n",
    "as the divisor in calculating spam probabilities. This adds another slight bias to protect against false positives.\n",
    "\"\"\"\n",
    "# Number of spam and non-spam e-mails messages (not words).\n",
    "number_bad_message = len(spam_corpus)\n",
    "number_good_messages = len(ham_corpus)\n",
    "\n",
    "# Threshold value for the spam filter algorithm (function individual_word_spam_chance).\n",
    "algorithm_threshold_value = 1\n",
    "# Threshold value to determine if the message is actually spam.\n",
    "spam_message_threshold_value = 0.9\n",
    "# Threshold value for the number of interesting tokens to use in testing for spam.\n",
    "interesting_tokens_threshold_value = 15\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "class SpamFilter:\n",
    "    \"\"\"\n",
    "    The SpamFilter class accepts a corpus and analyzes it to determine what words are spam and what words are\n",
    "    legitimate.\n",
    "\n",
    "    Note: Placeholder for now.\n",
    "\n",
    "    TODO - refactor spam filter to be class-based rather than just individual floating functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, corpus, threshold):\n",
    "        \"\"\"\n",
    "        Class constructor.\n",
    "\n",
    "        :param corpus: list of words to analyze.\n",
    "        :param threshold:  threshold value for the algorithm.\n",
    "        \"\"\"\n",
    "        self.corpus = corpus\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "def word_occurrences(corpus):\n",
    "    \"\"\"\n",
    "    Counts the number of times each word occurs in the message.\n",
    "\n",
    "    :param corpus: list of words to count occurrences for.\n",
    "    :return:  list of dictionaries of each word and their occurrences.\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    Here's a sketch of how I do statistical filtering.\n",
    "    I start with one corpus of spam and one of nonspam mail.\n",
    "    At the moment each one has about 4000 messages in it.\n",
    "    I scan the entire text, including headers and embedded html and javascript, of each message in each corpus.\n",
    "    I currently consider alphanumeric characters, dashes, apostrophes, and dollar signs to be part of tokens,\n",
    "    and everything else to be a token separator. (There is probably room for improvement here.)\n",
    "    I ignore tokens that are all digits, and I also ignore html comments,\n",
    "    not even considering them as token separators.\n",
    "\n",
    "    I count the number of times each token (ignoring case, currently) occurs in each corpus.\n",
    "    At this stage I end up with two large hash tables, one for each corpus, mapping tokens to number of occurrences.\n",
    "\n",
    "    \"\"\"\n",
    "    occur_array = []\n",
    "\n",
    "    for e in corpus:\n",
    "        occur = Counter(e)\n",
    "        occur_array.append(occur)\n",
    "\n",
    "    return occur_array\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "def convert_to_lowercase_words_only(spam_occurrences, nonspam_occurrences):\n",
    "    \"\"\"\n",
    "    Convert all keys to lower-case as words should not be case-sensitive.\n",
    "    Also, collapse all messages and their individual words into one combined dictionary containing\n",
    "    all unique component words and their occurrences.\n",
    "\n",
    "    :param spam_occurrences: contains all spam words and their occurrences as multiple dictionaries.\n",
    "    :param nonspam_occurrences: contains all non-spam words and their occurrences as multiple dictionaries.\n",
    "    :return:  two dictionaries, one for spam and one for non-spam, containing their lower-case words and\n",
    "    associated occurrences.\n",
    "\n",
    "    \"\"\"\n",
    "    lowercase_only_spam_word_occurrences = {}\n",
    "    for each_dictionary in spam_occurrences:\n",
    "        for key, value in each_dictionary.items():\n",
    "            if key.lower() not in lowercase_only_spam_word_occurrences:\n",
    "                lowercase_only_spam_word_occurrences[key.lower()] = value\n",
    "            else:\n",
    "                lowercase_only_spam_word_occurrences[key.lower()] += value\n",
    "    print(\"\\nlower-case only word spam word occurrences: \" + str(lowercase_only_spam_word_occurrences))\n",
    "\n",
    "    lowercase_only_nonspam_word_occurrences = {}\n",
    "    for each_dictionary in nonspam_occurrences:\n",
    "        for key, value in each_dictionary.items():\n",
    "            if key.lower() not in lowercase_only_nonspam_word_occurrences:\n",
    "                lowercase_only_nonspam_word_occurrences[key.lower()] = value\n",
    "            else:\n",
    "                lowercase_only_nonspam_word_occurrences[key.lower()] += value\n",
    "    print(\"lower-case only word non-spam word occurrences: \" + str(lowercase_only_nonspam_word_occurrences))\n",
    "\n",
    "    all_occurrences = [lowercase_only_spam_word_occurrences, lowercase_only_nonspam_word_occurrences]\n",
    "    return all_occurrences\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def individual_word_spam_chance(spam_words_dict, non_spam_words_dict, threshold):\n",
    "    \"\"\"\n",
    "    Determines the spam'liness of each individual word in the message.\n",
    "\n",
    "    :param spam_words_dict: dictionary containing spam words.\n",
    "    :param non_spam_words_dict: dictionary containing non-spam words.\n",
    "    :param threshold: threshold value for statistical algorithm.\n",
    "    :return:  spam'liness probability value of each individual word as a dictionary.\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "        Next I create a third hash table, this time mapping each token to the probability that an email containing\n",
    "    it is a spam, which I calculate as follows [1]:\n",
    "\n",
    "    (let ((g (* 2 (or (gethash word good) 0)))\n",
    "          (b (or (gethash word bad) 0)))\n",
    "       (unless (< (+ g b) 5)\n",
    "         (max .01\n",
    "              (min .99 (float (/ (min 1 (/ b nbad))\n",
    "                                 (+ (min 1 (/ g ngood))\n",
    "                                    (min 1 (/ b nbad)))))))))\n",
    "\n",
    "    where word is the token whose probability we're calculating,\n",
    "    good and bad are the hash tables I created in the first step,\n",
    "    and ngood and nbad are the number of nonspam and spam messages respectively.\n",
    "\n",
    "     ################################################################################\n",
    "\n",
    "    One question that arises in practice is what probability to assign to a word you've never seen, i.e.\n",
    "    one that doesn't occur in the hash table of word probabilities.\n",
    "    I've found, again by trial and error, that .4 is a good number to use.\n",
    "    If you've never seen a word before, it is probably fairly innocent; spam words tend to be all too familiar.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine unique spam and non-spam words into one dictionary with their associated combined occurrences.\n",
    "    combined_spam_nonspam_word_occurrences = {}\n",
    "\n",
    "    for key, value in spam_words_dict.items():\n",
    "        if key not in combined_spam_nonspam_word_occurrences:\n",
    "            combined_spam_nonspam_word_occurrences[key] = value\n",
    "        else:\n",
    "            combined_spam_nonspam_word_occurrences[key] += value\n",
    "    print(\"\\nlower-case only spam word occurrences as part of one combined dictionary: \"\n",
    "          + str(combined_spam_nonspam_word_occurrences))\n",
    "\n",
    "    for key, value in non_spam_words_dict.items():\n",
    "        if key not in combined_spam_nonspam_word_occurrences:\n",
    "            combined_spam_nonspam_word_occurrences[key] = value\n",
    "        else:\n",
    "            combined_spam_nonspam_word_occurrences[key] += value\n",
    "    print(\"lower-case only spam and non-spam word occurrences as part of one combined dictionary: \"\n",
    "          + str(combined_spam_nonspam_word_occurrences))\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # Iterate through all spam and non-spam words in combined dictionary and calculate spam probability for each word.\n",
    "    words_spam_chance = {}\n",
    "\n",
    "    for key, value in combined_spam_nonspam_word_occurrences.items():\n",
    "\n",
    "        # If word is not found in non-spam dictionary set value to 0, otherwise set to value found * 2.\n",
    "        if key not in non_spam_words_dict:\n",
    "            good_occurrences = 0\n",
    "        else:\n",
    "            good_occurrences = 2 * value\n",
    "\n",
    "        # If word is not found in spam dictionary set value to 0, otherwise set to that value found.\n",
    "        if key not in spam_words_dict:\n",
    "            bad_occurrences = 0\n",
    "        else:\n",
    "            bad_occurrences = value\n",
    "\n",
    "        # Statistical algorithm to calculate the associated probability for each word.\n",
    "        # Note to self: don't be an idiot and forget a parentheses messing up your order of operations.\n",
    "        if good_occurrences + bad_occurrences > threshold:\n",
    "            probability = max(0.01, min(0.99, min(1.0, bad_occurrences / number_bad_message) /\n",
    "                                        (min(1.0, good_occurrences / number_good_messages) +\n",
    "                                         min(1.0, bad_occurrences / number_bad_message))))\n",
    "        else:\n",
    "            probability = 0.0\n",
    "\n",
    "        # Store to dictionary each word and their associated probability.\n",
    "        words_spam_chance[key] = probability\n",
    "\n",
    "    # Return our dictionary of stored words spam probabilities.\n",
    "    return words_spam_chance\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "def find_interesting_tokens(test_corpus_words, word_spam_chance_dict):\n",
    "    \"\"\"\n",
    "    Prunes dictionary containing the words in the message to the most interesting 15 tokens based\n",
    "    on the size of their deviation from the \"Neutral\" value of 0.5\n",
    "\n",
    "    To perform this function, it first assigns spam probability to each words by referencing a established dataset\n",
    "    of words and their spam probability.  If word not found in dataset, assigned a fixed specific value.\n",
    "\n",
    "    :param test_corpus_words: the words in the message we wish to determine if it is spam.\n",
    "    :param word_spam_chance_dict: dictionary containing the spam probabilities of each word.\n",
    "    :return: the 15 most interesting words and their associated spam probabilities.\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    When new mail arrives, it is scanned into tokens, and the most interesting fifteen tokens,\n",
    "    where interesting is measured by how far their spam probability is from a neutral .5,\n",
    "    are used to calculate the probability that the mail is spam.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Assign probabilities to words in the test corpus based on established dataset of words and spam probabilities.\n",
    "    test_corpus_word_probability_dict = {}\n",
    "\n",
    "    for each_word in test_corpus_words:\n",
    "        # If dataset contains that word, assign previously calculated spam probability.\n",
    "        if each_word.lower() in word_spam_chance_dict.keys():\n",
    "            test_corpus_word_probability_dict[each_word.lower()] = word_spam_chance_dict[each_word.lower()]\n",
    "        # If dataset doesn't contain that word, assign specified spam probability value.\n",
    "        else:\n",
    "            test_corpus_word_probability_dict[each_word.lower()] = 0.4\n",
    "    print(\"\\ncontents of test corpus word probability dictionary: \" + str(test_corpus_word_probability_dict))\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # If more than 15 tokens, prune to the most \"interesting\" 15.\n",
    "    # Determine the 15 tokens with the largest deviation from neutral 0.5.\n",
    "    normalized_word_spam_chance = {}\n",
    "\n",
    "    for key, value in test_corpus_word_probability_dict.items():\n",
    "\n",
    "        # Prevent normalized values = 0.0.\n",
    "        if value == 0.5:\n",
    "            normalized_word_spam_chance[key] = abs(0.51 - value)\n",
    "        else:\n",
    "            normalized_word_spam_chance[key] = abs(0.5 - value)\n",
    "    print(\"\\nnormalized word spam chances: \" + str(normalized_word_spam_chance))\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # Sort dictionary so that largest deviations are at the front.\n",
    "    # FIXME - figure out a way to properly sort the dictionary based on value.\n",
    "    sorted_dict = sorted(normalized_word_spam_chance.items())\n",
    "    print(\"\\nsorted normalized word spam chances: \" + str(sorted_dict))\n",
    "\n",
    "    # Slice so only first 15 key-value pairs are left.\n",
    "    slice_dict = islice(sorted_dict, interesting_tokens_threshold_value)\n",
    "\n",
    "    # Convert to dictionary as islice returns an iterator.\n",
    "    first15 = {}\n",
    "    for each in slice_dict:\n",
    "        first15[each[0]] = each[1]\n",
    "    print(\"\\nfirst 15 tokens with normalized keys and values: \" + str(first15))\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # Un-normalize and return to original values by assigning original values.\n",
    "    first15_unnormalized = {}\n",
    "    for key, value in first15.items():\n",
    "        first15_unnormalized[key] = test_corpus_word_probability_dict[key]\n",
    "    print(\"first 15 tokens un-normalized keys and values: \" + str(first15_unnormalized))\n",
    "\n",
    "    return first15_unnormalized\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "def message_spam_chance(word_probabilities_dict):\n",
    "    \"\"\"\n",
    "    Determines the final probability of the message being spam.\n",
    "\n",
    "    :param word_probabilities_dict: individual probabilities for each word.\n",
    "    :return: probability the message is spam.\n",
    "\n",
    "    ################################################################################\n",
    "\n",
    "    If probs is a list of the fifteen individual probabilities, you calculate the combined probability thus:\n",
    "\n",
    "    (let ((prod (apply #'* probs)))\n",
    "      (/ prod (+ prod (apply #'* (mapcar #'(lambda (x)\n",
    "                                             (- 1 x))\n",
    "                                         probs)))))\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove the keys from the dictionary and store only the associated values.\n",
    "    word_probability_values = sorted(word_probabilities_dict.values())\n",
    "    print(\"\\nword spam probability values only, keys removed: \" + str(word_probability_values))\n",
    "\n",
    "    # Calculate the product of all individual word probabilities.\n",
    "    product_of_probabilities = 1.0\n",
    "    for each_probability in word_probability_values:\n",
    "        product_of_probabilities *= each_probability\n",
    "    print(\"product of individual values: \" + str(product_of_probabilities))\n",
    "\n",
    "    # Determine the complement value of all individual word probabilities.\n",
    "    word_probability_complement_values = []\n",
    "    for each_probability in word_probability_values:\n",
    "        complement = 1.00 - each_probability\n",
    "        word_probability_complement_values.append(complement)\n",
    "    print(\"word spam complement probability values: \" + str(word_probability_complement_values))\n",
    "\n",
    "    # Calculate the product of all complement probabilities.\n",
    "    product_of_complement_probabilities = 1.0\n",
    "    for each_complement_probability in word_probability_complement_values:\n",
    "        product_of_complement_probabilities *= each_complement_probability\n",
    "    print(\"product of complement values: \" + str(product_of_complement_probabilities))\n",
    "\n",
    "    spam_message_probability = product_of_probabilities / \\\n",
    "                               (product_of_probabilities + product_of_complement_probabilities)\n",
    "    print(\"final probability message is spam: \" + str(spam_message_probability))\n",
    "\n",
    "    return spam_message_probability\n",
    "\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Pithy Introduction.\n",
    "    Executes the program.\n",
    "    \"\"\"\n",
    "    print(\"\\nExecuting spam filter algorithm!\")\n",
    "    print(\"I like Spam! - Delicious!\")\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # Get occurrences of each word in the list of words - returned as list of dictionaries.\n",
    "    spamWordOccurrencesDict = word_occurrences(spam_corpus)\n",
    "    nonSpamWordOccurrencesDict = word_occurrences(ham_corpus)\n",
    "    print(\"\\noccurrences of each word in spam dictionary: \" + str(spamWordOccurrencesDict))\n",
    "    print(\"occurrences of each word in non-spam dictionary: \" + str(nonSpamWordOccurrencesDict))\n",
    "\n",
    "    # Convert words in each message to lower-case, sum occurrences of unique words in all messages,\n",
    "    # and return as list containing 2 dictionaries -one for spam and another for non-spam.\n",
    "    lower_case_words_only = convert_to_lowercase_words_only(spamWordOccurrencesDict, nonSpamWordOccurrencesDict)\n",
    "\n",
    "    # Determine probability that each word in the message is spam based on spam and non-spam corpus\n",
    "    # - returned as dictionary.\n",
    "    # TODO - this returns all the calculated probabilities for each word!!!!\n",
    "    word_spam_chance = individual_word_spam_chance(lower_case_words_only[0],\n",
    "                                                   lower_case_words_only[1], algorithm_threshold_value)\n",
    "    print(\"ALL WORDS SPAM PROBABILITIES (should make sense): \" + str(word_spam_chance))\n",
    "\n",
    "    ############################################\n",
    "    \"\"\"\n",
    "    This section is purely to use a test corpus to test for spam'liness against our established dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtain the 15 most interesting tokens in the message based on their normalized spam probabilities.\n",
    "    interesting_words_only = find_interesting_tokens(test_corpus[0], word_spam_chance)\n",
    "    # interesting_words_only = find_interesting_tokens(spam_corpus[0], word_spam_chance)\n",
    "    # interesting_words_only = find_interesting_tokens(spam_corpus[1], word_spam_chance)\n",
    "    # interesting_words_only = find_interesting_tokens(ham_corpus[0], word_spam_chance)\n",
    "    # interesting_words_only = find_interesting_tokens(ham_corpus[1], word_spam_chance)\n",
    "\n",
    "    # Obtain the spam message final probability value.\n",
    "    result = message_spam_chance(interesting_words_only)\n",
    "\n",
    "    # Compare spam message final probability value against threshold spam probability value.\n",
    "    print(\"\\nThreshold value above which e-mail message is considered spam: \" + str(spam_message_threshold_value))\n",
    "    if result >= spam_message_threshold_value:\n",
    "        print(\"Spam!!!!\")\n",
    "    else:\n",
    "        print(\"Not spam.\")\n",
    "\n",
    "############################################################################################\n",
    "############################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Graham argues that this is a Bayesian approach to SPAM. What makes it Bayesian?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike feature-recognizing filters which assigns an arbitrary spam \"score\" to an e-mail, the Bayaesian approach assigns actual probabilities.  Therefore, you know what you are measuring and there is little ambiguity about how the evidence should be combined to calculate the chances of the e-mail being spam.\n",
    "\n",
    "The Bayesian approach considers all evidence, the good evidence decreasing the likelihood of the message being spam and the bad evidence increasing the likelihood of the message being spam.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian network used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cs.calvin.edu/courses/cs/344/kvlinden/05bayesnets/images/figure14_12.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://cs.calvin.edu/courses/cs/344/kvlinden/05bayesnets/images/figure14_12.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "P(Cloudy)\n",
      "False: 0.5, True: 0.5\n",
      "False: 0.5, True: 0.5\n",
      "False: 0.537, True: 0.463\n",
      "False: 0.505, True: 0.495\n",
      "False: 0.494, True: 0.506\n",
      "\n",
      "P(Sprinkler | cloudy)\n",
      "False: 0.9, True: 0.1\n",
      "False: 0.9, True: 0.1\n",
      "False: 0.903, True: 0.097\n",
      "False: 0.908, True: 0.0918\n",
      "False: 0.899, True: 0.101\n",
      "\n",
      "P(Cloudy| the sprinkler is running and it’s not raining)\n",
      "False: 0.952, True: 0.0476\n",
      "False: 0.952, True: 0.0476\n",
      "False: 0.961, True: 0.039\n",
      "False: 0.952, True: 0.0478\n",
      "False: 0.952, True: 0.0475\n",
      "\n",
      "P(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\n",
      "False: 0.01, True: 0.99\n",
      "False: 0.01, True: 0.99\n",
      "False: 0.012, True: 0.988\n",
      "False: 0.00943, True: 0.991\n",
      "False: 0.0099, True: 0.99\n",
      "\n",
      "P(Cloudy | the grass is not wet)\n",
      "False: 0.639, True: 0.361\n",
      "False: 0.639, True: 0.361\n",
      "False: 0.647, True: 0.353\n",
      "False: 0.648, True: 0.352\n",
      "False: 0.645, True: 0.355\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This module implements the Bayesian network shown in the text, Figure 14.2.\n",
    "It's taken from the AIMA Python code.\n",
    "\n",
    "@author: kvlinden\n",
    "@version Jan 2, 2013\n",
    "'''\n",
    "\n",
    "from probability import BayesNet, enumeration_ask, elimination_ask, gibbs_ask, rejection_sampling, likelihood_weighting\n",
    "\n",
    "# Utility variables\n",
    "T, F = True, False\n",
    "\n",
    "# From AIMA code (probability.py) - Fig. 14.2 - burglary example\n",
    "cloudy = BayesNet([\n",
    "    ('Cloudy', '', 0.50),\n",
    "    ('Rain', 'Cloudy', {T: 0.80, F: 0.20}),\n",
    "    ('Sprinkler', 'Cloudy', {T: 0.10, F: 0.50}),\n",
    "    ('WetGrass', 'Sprinkler Rain', {(T, T): 0.99, (T, F): 0.90, (F, T): 0.90, (F, F): 0.00})\n",
    "    ])\n",
    "\n",
    "# Compute P(Cloudy)\n",
    "print(\"\\nP(Cloudy)\")\n",
    "print(enumeration_ask('Cloudy', dict(), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('Cloudy', dict(), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('Cloudy', dict(), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('Cloudy', dict(), cloudy).show_approx())\n",
    "print(likelihood_weighting('Cloudy', dict(), cloudy).show_approx())\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Compute P(Sprinkler | cloudy)\n",
    "print(\"\\nP(Sprinkler | cloudy)\")\n",
    "print(enumeration_ask('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "print(likelihood_weighting('Sprinkler', dict(Cloudy=T), cloudy).show_approx())\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Compute P(Cloudy| the sprinkler is running and it’s not raining)\n",
    "print(\"\\nP(Cloudy| the sprinkler is running and it’s not raining)\")\n",
    "print(enumeration_ask('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "print(likelihood_weighting('Cloudy', dict(Sprinkler=T, Rain=F), cloudy).show_approx())\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Compute P(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\n",
    "print(\"\\nP(WetGrass | it’s cloudy, the sprinkler is running and it’s raining)\")\n",
    "print(enumeration_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "print(likelihood_weighting('WetGrass', dict(Cloudy=T, Sprinkler=T, Rain=T), cloudy).show_approx())\n",
    "\n",
    "###################################################################################################\n",
    "\n",
    "# Compute P(Cloudy | the grass is not wet)\n",
    "print(\"\\nP(Cloudy | the grass is not wet)\")\n",
    "print(enumeration_ask('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "# elimination_ask() is a dynamic programming version of enumeration_ask().\n",
    "print(elimination_ask('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "# gibbs_ask() is an approximation algorithm helps Bayesian Networks scale up.\n",
    "print(gibbs_ask('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "# See the explanation of the algorithms in AIMA Section 14.4.\n",
    "print(rejection_sampling('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "print(likelihood_weighting('Cloudy', dict(WetGrass=F), cloudy).show_approx())\n",
    "\n",
    "# TODO - fix module not found error in jupyter notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand calculated probabilities for Bayesian network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Refer to screen captures in Homework 2 directory and/or turned in hard-copy of hand calculations. (will enter hand-calculations using latex syntax if/when I have time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center; transform: rotate(90deg); margin: 0 10px 10px 0\" src=\"bayesian_network_hand_calc_page1_revised.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center; transform: rotate(90deg); margin: 0 10px 10px 0\" src=\"bayesian_network_hand_calc_page2.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center; transform: rotate(90deg); margin: 0 10px 10px 0\" src=\"bayesian_network_hand_calc_page3.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: TODO - Images are cut-off for some reason.  Fix it.\n",
    "\n",
    "Note: TODO - Given time, convert all hand calculations to Latex format following the below examples (looking unlikely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example *causal* computation** \n",
    "\n",
    "This computes the probability that John calls given that there as been a burglary but no earthquake.\n",
    "\n",
    "$\\begin{aligned}\n",
    "    \\textbf{P}(J | b \\land \\neg e)\n",
    "        &= \\alpha \\sum_a{\\textbf{P}(J, a, b, \\neg e)} \\\\\n",
    "        &= \\alpha \\sum_a{\\textbf{P}(J|a) \\cdot P(a|b,\\neg e) \\cdot P(b) \\cdot P(\\neg e)} \\\\\n",
    "        &= \\alpha \\cdot P(b) \\cdot P(\\neg e) \\cdot \\sum_a{P(J|a) \\cdot P(a|b,-e)} \\\\\n",
    "        &= \\alpha \\cdot 0.001 \\cdot 0.998 \\cdot \\langle(0.9 \\cdot 0.94 + 0.05 \\cdot 0.06), (0.1 \\cdot 0.94 + 0.95 \\cdot 0.06)\\rangle \\\\\n",
    "        &= \\alpha \\langle0.000847, 0.000150698\\rangle \\\\\n",
    "        &= \\langle0.85, 0.15\\rangle\n",
    "    \\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example *diagnostic* computation** \n",
    "\n",
    "This computes the probability that there as been a burglary given that the alarm has rung.\n",
    "\n",
    "$\\begin{aligned}\n",
    "                \\textbf{P}(B | a)\n",
    "                    &= \\alpha \\sum_e{\\textbf{P}(B, e, a)} \\\\\n",
    "                    &= \\alpha \\sum_e{\\textbf{P}(B) * P(e) * P(a | B, e)} \\\\\n",
    "                    &= \\alpha * \\langle (0.001 * 0.002 * 0.95 + 0.001 * 0.998 * 0.94), (0.999 * 0.002 * 0.29 + 0.999 * 0.998 * 0.001)\\rangle \\\\\n",
    "                    &= \\alpha * \\langle0.00094002, 0.001576422\\rangle \\\\\n",
    "                    &= \\langle0.374, 0.626\\rangle\n",
    "                \\end{aligned}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: Compute the number of independent values in the full joint probability distribution for this domain. Assume that no conditional independence relations are known to hold between these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Ensure my answer is correct.\n",
    "\n",
    "There are n=4 binary random variables, therefore the number of independent parameters is 2^(n=4)-1.  This results in 2^4-1 = 15 independent parameters in total.\n",
    "\n",
    "Resources Used:\n",
    "\n",
    "http://www.cs.brandeis.edu/~cs134/K_F_Ch3.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the number of independent values in the Bayesian network for this domain. Assume the conditional independence relations implied by the Bayes network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Ensure my answer is correct.\n",
    "\n",
    "There are n=4 binary random variables, therefore the number of independent parameters is 2*(n=4)-1.  This results in 2*4-1 = 7 independent parameters in total.\n",
    "\n",
    "Resources Used:\n",
    "\n",
    "http://www.cs.brandeis.edu/~cs134/K_F_Ch3.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
