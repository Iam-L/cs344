{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# CS-344 Artificial Intelligence - Final Project\n",
    "## Project: SLO TBL Topic Classification\n",
    "## Author: Joseph Jinn\n",
    "## Date: 5-3-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Final Project Proposal - Draft 2 - Vision Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "A short summary and background of the project."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "Project Domain:\n",
    "\n",
    "\tUse Keras to implement deep neural networks to do SLO topic classification over the standard TBL topics using Tweets relating to mining companies:\n",
    "\n",
    "    \tEnvironmental\n",
    "    \tEconomic \n",
    "    \tSocial \n",
    "\n",
    "\tSocial License to Operate (SLO) – refers to the ongoing acceptance of a company or industry’s standard business practices and operating procedures by its employees, stakeholders, and the general public.\n",
    "\n",
    "\tTriple Bottom Line (TBL) – a framework or theory that recommends that companies commit to focus on social and environmental concerns just as they do on profits.\n",
    "    \n",
    "    \tGauges a corporation’s level of commitment to corporate social responsibility and its impact on the environment over time.\n",
    "\n",
    "\tResources:\n",
    "    \thttps://www.investopedia.com/terms/s/social-license-slo.asp\n",
    "    \thttps://www.investopedia.com/terms/t/triple-bottom-line.asp\n",
    "    \tClassifying Stance Using Profile Texts – Anonymous ACL submission\n",
    "\n",
    "Project Ideas:\n",
    "\n",
    "\tIf necessary, manually hand-tag a new dataset to create training, validation, and test subsets for use in SLO topic classification.\n",
    "\tEnable GPU support for Keras Tensorflow back-end for training the model.\n",
    "\tParallelize GPU support for Keras Tensorflow back-end for training the model.\n",
    "\tI guess what I’ll actually be doing will depend on what the next step is in taking this idea further.\n",
    "\n",
    "\n",
    "\n",
    "Framing the Problem (from Google Machine Learning Problem Framing):\n",
    "\n",
    "The machine learning model should be able to correctly identify whether the Tweet is positively oriented or negatively oriented as an environmental topic, economic topic, or social topic.\n",
    "\n",
    "The ideal outcome is such that the model predicts with 90%+ confidence the orientation (positive or negative) and type of topic (environmental, economic, or social) of the Tweet.\n",
    "\n",
    "The success metric is that the probability value for the correct label is the highest among all values given by a softmax layer.\n",
    "\n",
    "The model is deemed a failure if the probability value for the correct label is not the highest among all values given by a softmax layer.\n",
    "\n",
    "The output of the machine learning model will be an array of probabilities, providing confidence values in how sure the model is that the Tweet is one of the triple-bottom-line topics.  The values should sum to a total of 1.\n",
    "\n",
    "The results will be used to help determine the Social License to Operate (SLO) of mining companies, where SLO is a measure of the company’s level of support from their constituencies.\n",
    "\n",
    "\n",
    "\n",
    "Formulating the Problem (from Google Machine Learning Problem Framing):\n",
    "\n",
    "The problem is best suited to the use of a multi-class single label classification training model.\n",
    "\n",
    "Simplifying, the training model should output for each class the probability that the Tweet belongs to that class.  This process should be performed for all classes that define all possible SLO classifications.\n",
    "\n",
    "Data (from Classifying Stance Using Profile Texts – Anonymous ACL submission .pdf document):\n",
    "\n",
    "Twitter data – raw Tweets collected from the Twitter API.\n",
    "\n",
    "\tPre-processing:\n",
    "    \tTokenize texts using the CMU Tweet Tagger.  Stop words are retained.\n",
    "    \tRemove “RT” tags marking re-tweets.\n",
    "    \tShrink character elongations except in usernames.\n",
    "    \tReplace URL’s, mentions, year/time/cash/hashtag items with placeholders.\n",
    "    \tDown-case all text.\n",
    "    \tRemove tweets that are not labelled as some variant of English either by the Twitter or by Polyglot.\n",
    "    \tRemove the tweets found to not be associated with any company.\n",
    "\n",
    "\tInputs:\n",
    "    \tInput 1: N-gram counts.\n",
    "    \tInput 2: target company name presence.\n",
    "    \tInput 3: word embeddings.\n",
    "\n",
    "\tOutputs:\n",
    "    \tSLO classification:\n",
    "    \tEnvironmental\n",
    "    \tEconomic\n",
    "    \tSocial\n",
    "\n",
    "We should initially start with:\n",
    "    \tInput 1: N-gram counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Final Project Proposal - Draft 2 - Code-Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## SLO_TBL_Tweet_Preprocessor_Specialized.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "This Python program pre-processes several datasets in preparation for TBL topic classification using Scikit-Learn Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Time taken to run pre-processor function:\n",
      "DEBUG:root:0.0\n",
      "DEBUG:root:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Course: CS 344 - Artificial Intelligence\n",
    "Instructor: Professor VanderLinden\n",
    "Name: Joseph Jinn\n",
    "Date: 4-23-19\n",
    "\n",
    "Final Project - SLO TBL Topic Classification\n",
    "\n",
    "###########################################################\n",
    "Notes:\n",
    "\n",
    "These function(s) performs Tweet pre-processing SPECIFIC TO A SINGLE DATASET ONLY.\n",
    "This is NOT a generalized Tweet dataset preprocessor!!!\n",
    "\n",
    "###########################################################\n",
    "Resources Used:\n",
    "\n",
    "Refer to slo_topic_classification_original.py for a full list of URL's to online resources referenced.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import logging as log\n",
    "\n",
    "# Note: Need to set level AND turn on debug variables in order to see all debug output.\n",
    "log.basicConfig(level=log.DEBUG)\n",
    "\n",
    "# Miscellaneous parameter adjustments for pandas and python.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# Turn on and off to debug various sub-sections.\n",
    "debug = True\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def tweet_dataset_preprocessor_1():\n",
    "    \"\"\"\n",
    "    Function pre-processes tbl_training_set.csv in preparation for machine learning input feature creation.\n",
    "\n",
    "    :return: Nothing. Saves to CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the dataset.\n",
    "    slo_dataset = \\\n",
    "        pd.read_csv(\"tbl-datasets/tbl_training_set.csv\", sep=\",\")\n",
    "\n",
    "    # Shuffle the data randomly.\n",
    "    slo_dataset = slo_dataset.reindex(\n",
    "        pd.np.random.permutation(slo_dataset.index))\n",
    "\n",
    "    # Rename columns to something that makes sense.\n",
    "    column_names = ['Tweet', 'SLO1', 'SLO2', 'SLO3']\n",
    "\n",
    "    # Generate a Pandas dataframe.\n",
    "    slo_dataframe = pd.DataFrame(slo_dataset)\n",
    "\n",
    "    if debug:\n",
    "        # Print shape and column names.\n",
    "        log.debug(\"The shape of our SLO dataframe:\")\n",
    "        log.debug(slo_dataframe.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The columns of our SLO dataframe:\")\n",
    "        log.debug(slo_dataframe.head)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Assign column names.\n",
    "    slo_dataframe.columns = column_names\n",
    "\n",
    "    ################################################################################################################\n",
    "\n",
    "    # Drop all rows with only NaN in all columns.\n",
    "    slo_dataframe = slo_dataframe.dropna(how='all')\n",
    "    # Drop all rows without at least 2 non NaN values - indicating no existing SLO TBL topic classification labels.\n",
    "    slo_dataframe = slo_dataframe.dropna(thresh=2)\n",
    "\n",
    "    if debug:\n",
    "        # Iterate through each row and check we dropped properly.\n",
    "        log.debug(\"Dataframe with only examples that have SLO TBL topic classification labels:\")\n",
    "        for index in slo_dataframe.index:\n",
    "            log.debug(slo_dataframe['Tweet'][index] + '\\tSLO1: ' + str(slo_dataframe['SLO1'][index])\n",
    "                      + '\\tSLO2: ' + str(slo_dataframe['SLO2'][index]) + '\\tSLO3: ' + str(slo_dataframe['SLO3'][index]))\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\n",
    "            \"Shape of dataframe with only examples that have SLO TBL topic classifications: \" + str(\n",
    "                slo_dataframe.shape))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Boolean indexing to select examples with only a single SLO TBL topic classification.\n",
    "    mask = slo_dataframe['SLO1'].notna() & (slo_dataframe['SLO2'].isna() & slo_dataframe['SLO3'].isna())\n",
    "\n",
    "    if debug:\n",
    "        # Check that boolean indexing is working.\n",
    "        log.debug(\"Check that our boolean indexing mask gives only examples with a single SLO TBL topic \"\n",
    "                  \"classification:\")\n",
    "        log.debug(mask.tail)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The shape of our boolean indexing mask:\")\n",
    "        log.debug(mask.shape)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Create new dataframe with examples that have only a single SLO TBL topic classification.\n",
    "    slo_dataframe_single_classification = slo_dataframe[mask]\n",
    "\n",
    "    if debug:\n",
    "        # Check that we have created the new dataframe properly.\n",
    "        log.debug(\"Dataframe with only examples that have a single SLO TBL topic classification labels:\")\n",
    "        # Iterate through each row and check that only examples with a single SLO TBL topic classification are left.\n",
    "        for index in slo_dataframe_single_classification.index:\n",
    "            log.debug(slo_dataframe_single_classification['Tweet'][index]\n",
    "                      + '\\tSLO1: ' + str(slo_dataframe_single_classification['SLO1'][index])\n",
    "                      + '\\tSLO2: ' + str(slo_dataframe_single_classification['SLO2'][index])\n",
    "                      + '\\tSLO3: ' + str(slo_dataframe_single_classification['SLO3'][index]))\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"Shape of dataframe with only examples that have a single SLO TBL topic classification: \"\n",
    "                  + str(slo_dataframe_single_classification.shape))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Drop SLO2 and SLO3 columns as they are just NaN values.\n",
    "    slo_dataframe_single_classification = slo_dataframe_single_classification.drop(columns=['SLO2', 'SLO3'])\n",
    "\n",
    "    if debug:\n",
    "        # Check that we have dropped SLO2 and SLO3 columns properly.\n",
    "        log.debug(\"Dataframe with SLO2 and SLO3 columns dropped as they are just NaN values:\")\n",
    "        # Iterate through each row and check that each example only has one SLO TBL topic classification left.\n",
    "        for index in slo_dataframe_single_classification.index:\n",
    "            log.debug(slo_dataframe_single_classification['Tweet'][index] + '\\tSLO1: '\n",
    "                      + str(slo_dataframe_single_classification['SLO1'][index]))\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\n",
    "            \"Shape of dataframe with SLO2 and SLO3 columns dropped: \" + str(slo_dataframe_single_classification.shape))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Re-name columns for clarity of purpose.\n",
    "    column_names_single = ['Tweet', 'SLO']\n",
    "    slo_dataframe_single_classification.columns = column_names_single\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Boolean indexing to select examples with multiple SLO TBL topic classifications.\n",
    "    mask = slo_dataframe['SLO1'].notna() & (slo_dataframe['SLO2'].notna() | slo_dataframe['SLO3'].notna())\n",
    "\n",
    "    if debug:\n",
    "        # Check that boolean indexing is working.\n",
    "        log.debug(\n",
    "            \"Check that our boolean indexing mask gives only examples with multiple SLO TBL topic classifications:\")\n",
    "        log.debug(mask.tail)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The shape of our boolean indexing mask:\")\n",
    "        log.debug(mask.shape)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Create new dataframe with only those examples with multiple SLO TBL topic classifications.\n",
    "    slo_dataframe_multiple_classifications = slo_dataframe[mask]\n",
    "\n",
    "    if debug:\n",
    "        # Check that we have created the new dataframe properly.\n",
    "        log.debug(\"Dataframe with only examples that have multiple SLO TBL topic classification labels:\")\n",
    "        # Iterate through each row and check that only examples with multiple SLO TBL topic classifications are left.\n",
    "        for index in slo_dataframe_multiple_classifications.index:\n",
    "            log.debug(slo_dataframe_multiple_classifications['Tweet'][index]\n",
    "                      + '\\tSLO1: ' + str(slo_dataframe_multiple_classifications['SLO1'][index])\n",
    "                      + '\\tSLO2: ' + str(slo_dataframe_multiple_classifications['SLO2'][index])\n",
    "                      + '\\tSLO3: ' + str(slo_dataframe_multiple_classifications['SLO3'][index]))\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"Shape of dataframe with only examples that have multiple SLO TBL topic classifications: \"\n",
    "                  + str(slo_dataframe_multiple_classifications.shape))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Duplicate examples with multiple SLO TBL classifications into examples with only 1 SLO TBL topic classification\n",
    "    # each.\n",
    "    slo1_dataframe = slo_dataframe_multiple_classifications.drop(columns=['SLO2', 'SLO3'])\n",
    "    slo2_dataframe = slo_dataframe_multiple_classifications.drop(columns=['SLO1', 'SLO3'])\n",
    "    slo3_dataframe = slo_dataframe_multiple_classifications.drop(columns=['SLO1', 'SLO2'])\n",
    "\n",
    "    if debug:\n",
    "        # Check that we have created the new dataframes properly.\n",
    "        log.debug(\n",
    "            \"Separated dataframes with only a single label for examples with multiple SLO TBL topic classification \"\n",
    "            \"labels:\")\n",
    "        # Iterate through each row and check that each example only has one SLO TBL topic classification left.\n",
    "        for index in slo1_dataframe.index:\n",
    "            log.debug(slo1_dataframe['Tweet'][index] + '\\tSLO1: ' + str(slo1_dataframe['SLO1'][index]))\n",
    "        for index in slo2_dataframe.index:\n",
    "            log.debug(slo2_dataframe['Tweet'][index] + '\\tSLO2: ' + str(slo2_dataframe['SLO2'][index]))\n",
    "        for index in slo3_dataframe.index:\n",
    "            log.debug(slo3_dataframe['Tweet'][index] + '\\tSLO3: ' + str(slo3_dataframe['SLO3'][index]))\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"Shape of slo1_dataframe: \" + str(slo1_dataframe.shape))\n",
    "        log.debug(\"Shape of slo2_dataframe: \" + str(slo2_dataframe.shape))\n",
    "        log.debug(\"Shape of slo3_dataframe: \" + str(slo3_dataframe.shape))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Re-name columns for clarity of purpose.\n",
    "    column_names_single = ['Tweet', 'SLO']\n",
    "\n",
    "    slo1_dataframe.columns = column_names_single\n",
    "    slo2_dataframe.columns = column_names_single\n",
    "    slo3_dataframe.columns = column_names_single\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Concatenate the individual dataframes back together.\n",
    "    frames = [slo1_dataframe, slo2_dataframe, slo3_dataframe, slo_dataframe_single_classification]\n",
    "    slo_dataframe_combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Note: Doing this as context-sensitive menu stopped displaying all useable function calls after concat.\n",
    "    slo_dataframe_combined = pd.DataFrame(slo_dataframe_combined)\n",
    "\n",
    "    if debug:\n",
    "        # Check that we have recombined the dataframes properly.\n",
    "        log.debug(\"Recombined individual dataframes for the dataframe representing Tweets with only a single SLO TBL \"\n",
    "                  \" topic classification\\nand for the dataframes representing Tweets with multiple SLO TBL topic\"\n",
    "                  \"classifications:\")\n",
    "        # Iterate through each row and check that each example only has one SLO TBL Classification left.\n",
    "        for index in slo_dataframe_combined.index:\n",
    "            log.debug(slo_dataframe_combined['Tweet'][index] + '\\tSLO: ' + str(slo_dataframe_combined['SLO'][index]))\n",
    "        log.debug('\\n')\n",
    "        log.debug('Shape of recombined dataframes: ' + str(slo_dataframe_combined.shape))\n",
    "        log.debug('\\n')\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Drop all rows with only NaN in all columns.\n",
    "    slo_dataframe_combined = slo_dataframe_combined.dropna()\n",
    "\n",
    "    if debug:\n",
    "        # Check that we have dropped all NaN's properly.\n",
    "        log.debug(\"Recombined dataframes - NaN examples removed:\")\n",
    "        # Iterate through each row and check that we no longer have examples with NaN values.\n",
    "        for index in slo_dataframe_combined.index:\n",
    "            log.debug(slo_dataframe_combined['Tweet'][index] + '\\tSLO: ' + str(slo_dataframe_combined['SLO'][index]))\n",
    "        log.debug('\\n')\n",
    "        log.debug('Shape of recombined dataframes without NaN examples: ' + str(slo_dataframe_combined.shape))\n",
    "        log.debug('\\n')\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Drop duplicate examples with the same SLO TBL topic classification class.\n",
    "    slo_dataframe_tbl_duplicates_dropped = slo_dataframe_combined.drop_duplicates(subset=['Tweet', 'SLO'], keep=False)\n",
    "\n",
    "    if debug:\n",
    "        # Check that we have dropped all duplicate labels properly.\n",
    "        log.debug(\"Duplicate examples with duplicate SLO TBL topic classifications removed:\")\n",
    "        # Iterate through each row and check that we no longer have duplicate examples with the same labels.\n",
    "        for index in slo_dataframe_tbl_duplicates_dropped.index:\n",
    "            log.debug(slo_dataframe_tbl_duplicates_dropped['Tweet'][index] + '\\tSLO: '\n",
    "                      + str(slo_dataframe_tbl_duplicates_dropped['SLO'][index]))\n",
    "        log.debug('\\n')\n",
    "        log.debug(\n",
    "            'Shape of dataframes without duplicate TBL labels: ' + str(slo_dataframe_tbl_duplicates_dropped.shape))\n",
    "        log.debug('\\n')\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    def preprocess_tweet_text(tweet_text):\n",
    "        \"\"\"\n",
    "        Helper function performs text pre-processing using regular expressions and other Python functions.\n",
    "\n",
    "        Notes:\n",
    "\n",
    "        Stop words are retained.\n",
    "\n",
    "        TODO - shrink character elongations\n",
    "        TODO - remove non-english tweets\n",
    "        TODO - remove non-company associated tweets\n",
    "        TODO - remove year and time.\n",
    "        TODO - remove cash items?\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Remove \"RT\" tags.\n",
    "        preprocessed_tweet_text = re.sub(\"rt\", \"\", tweet_text)\n",
    "\n",
    "        # Remove URL's.\n",
    "        preprocessed_tweet_text = re.sub(\"http[s]?://\\S+\", \"slo_url\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove Tweet mentions.\n",
    "        preprocessed_tweet_text = re.sub(\"@\\S+\", \"slo_mention\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove Tweet hashtags.\n",
    "        preprocessed_tweet_text = re.sub(\"#\\S+\", \"slo_hashtag\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove all punctuation.\n",
    "        preprocessed_tweet_text = preprocessed_tweet_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        return preprocessed_tweet_text\n",
    "\n",
    "    # Assign new dataframe to contents of old.\n",
    "    slo_df_tokenized = slo_dataframe_tbl_duplicates_dropped\n",
    "\n",
    "    # Down-case all text.\n",
    "    slo_df_tokenized['Tweet'] = slo_df_tokenized['Tweet'].str.lower()\n",
    "\n",
    "    # Pre-process each tweet individually.\n",
    "    for index in slo_df_tokenized.index:\n",
    "        slo_df_tokenized['Tweet'][index] = preprocess_tweet_text(slo_df_tokenized['Tweet'][index])\n",
    "\n",
    "    # Reindex everything.\n",
    "    slo_df_tokenized.index = pd.RangeIndex(len(slo_df_tokenized.index))\n",
    "    # slo_df_tokenized.index = range(len(slo_df_tokenized.index))\n",
    "\n",
    "    # Save to CSV file.\n",
    "    slo_df_tokenized.to_csv(\"preprocessed-datasets/tbl_training_set_PROCESSED.csv\", sep=',',\n",
    "                            encoding='utf-8', index=False, header=['Tweet', 'SLO'])\n",
    "\n",
    "    # return slo_df_tokenized\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def tweet_dataset_preprocessor_2():\n",
    "    \"\"\"\n",
    "      Function pre-processes tbl_kvlinden.csv in preparation for machine learning input feature creation.\n",
    "\n",
    "    :return: Nothing. Saves to CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the dataset.\n",
    "    slo_dataset = \\\n",
    "        pd.read_csv(\"tbl-datasets/tbl_kvlinden.csv\", sep=\",\")\n",
    "\n",
    "    # Shuffle the data randomly.\n",
    "    slo_dataset = slo_dataset.reindex(\n",
    "        pd.np.random.permutation(slo_dataset.index))\n",
    "\n",
    "    # Rename columns to something that makes sense.\n",
    "    column_names = ['Tweet', 'SLO1', 'SLO2']\n",
    "\n",
    "    # Generate a Pandas dataframe.\n",
    "    slo_dataframe = pd.DataFrame(slo_dataset)\n",
    "\n",
    "    if debug:\n",
    "        # Print shape and column names.\n",
    "        log.debug(\"The shape of our SLO dataframe:\")\n",
    "        log.debug(slo_dataframe.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The columns of our SLO dataframe:\")\n",
    "        log.debug(slo_dataframe.head)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The 2nd column of our SLO dataframe:\")\n",
    "\n",
    "    # Assign column names.\n",
    "    slo_dataframe.columns = column_names\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"The Tweets column:\")\n",
    "        log.debug(slo_dataframe['Tweet'])\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The SLO column:\")\n",
    "        log.debug(slo_dataframe['SLO1'])\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The 2nd SLO column:\")\n",
    "        log.debug(slo_dataframe['SLO2'])\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Restrict to just SLO1 column by dropping SLO2 column.\n",
    "    slo_dataframe_column1 = slo_dataframe.drop('SLO2', axis=1)\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"The shape of dataframe with only slo column1:\")\n",
    "        log.debug(slo_dataframe_column1.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The contents of the dataframe with only slo column1:\")\n",
    "        log.debug(slo_dataframe_column1.sample())\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Drop any row with \"NaN\" columns. (isolates examples with multiple TBL classification labels)\n",
    "    slo_dataframe_column2 = slo_dataframe.dropna()\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"The contents of the dataframe with only examples containing multiple classifications\")\n",
    "        for index in slo_dataframe_column2.index:\n",
    "            log.debug(slo_dataframe_column2['Tweet'][index] + '\\tSLO1: '\n",
    "                      + str(slo_dataframe_column2['SLO1'][index])\n",
    "                      + '\\tSLO2: ' + str(slo_dataframe_column2['SLO2'][index]))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Drop SLO1 column to restrict to just 2nd classification label in SLO2 column.\n",
    "    slo_dataframe_column2 = slo_dataframe_column2.drop('SLO1', axis=1)\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Rename columns for concatenation back into a single dataframe.\n",
    "    column_names = [\"Tweet\", \"SLO\"]\n",
    "    slo_dataframe_column1.columns = column_names\n",
    "    slo_dataframe_column2.columns = column_names\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"Check that we have renamed columns properly:\")\n",
    "        log.debug(slo_dataframe_column1.head())\n",
    "        log.debug(slo_dataframe_column2.head())\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Concatenate the individual dataframes back together.\n",
    "    frames = [slo_dataframe_column1, slo_dataframe_column2]\n",
    "    slo_dataframe_combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"Check that we have concatenated properly:\")\n",
    "        log.debug(slo_dataframe_combined.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(slo_dataframe_combined.tail())\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    def preprocess_tweet_text(tweet_text):\n",
    "        \"\"\"\n",
    "        Helper function performs text pre-processing using regular expressions and other Python functions.\n",
    "\n",
    "        Notes:\n",
    "\n",
    "        Stop words are retained.\n",
    "\n",
    "        TODO - shrink character elongations\n",
    "        TODO - remove non-english tweets\n",
    "        TODO - remove non-company associated tweets\n",
    "        TODO - remove year and time.\n",
    "        TODO - remove cash items?\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Remove \"RT\" tags.\n",
    "        preprocessed_tweet_text = re.sub(\"rt\", \"\", tweet_text)\n",
    "\n",
    "        # Remove URL's.\n",
    "        preprocessed_tweet_text = re.sub(\"http[s]?://\\S+\", \"slo_url\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove Tweet mentions.\n",
    "        preprocessed_tweet_text = re.sub(\"@\\S+\", \"slo_mention\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove Tweet hashtags.\n",
    "        preprocessed_tweet_text = re.sub(\"#\\S+\", \"slo_hashtag\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove all punctuation.\n",
    "        preprocessed_tweet_text = preprocessed_tweet_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        return preprocessed_tweet_text\n",
    "\n",
    "        # Assign new dataframe to contents of old.\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Down-case all text.\n",
    "    slo_dataframe_combined['Tweet'] = slo_dataframe_combined['Tweet'].str.lower()\n",
    "\n",
    "    # Pre-process each tweet individually.\n",
    "    for index in slo_dataframe_combined.index:\n",
    "        slo_dataframe_combined['Tweet'][index] = preprocess_tweet_text(slo_dataframe_combined['Tweet'][index])\n",
    "\n",
    "    # Reindex everything.\n",
    "    slo_dataframe_combined.index = pd.RangeIndex(len(slo_dataframe_combined.index))\n",
    "    # slo_dataframe_combined.index = range(len(slo_dataframe_combined.index))\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"Check that we have pre-processed properly:\")\n",
    "        for index in slo_dataframe_combined.index:\n",
    "            log.debug(slo_dataframe_combined['Tweet'][index] + '\\tSLO: '\n",
    "                      + str(slo_dataframe_combined['SLO'][index]))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Save to CSV file.\n",
    "    slo_dataframe_combined.to_csv(\"preprocessed-datasets/tbl_kvlinden_PROCESSED.csv\", sep=',',\n",
    "                                  encoding='utf-8', index=False, header=['Tweet', 'SLO'])\n",
    "\n",
    "    # return slo_dataframe_combined\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def tweet_dataset_preprocessor_3():\n",
    "    \"\"\"\n",
    "    Function pre-processes dataset_20100101-20180510_tok.csv in preparation for machine learning input feature creation.\n",
    "\n",
    "    Note: We are doing this as our pre-processing for the other datasets we are using is different from the\n",
    "    pre-processing done on this already tokenized dataset.  Hence, we wish to normalize the difference between them\n",
    "    as much as possible before using this dataset as our prediction set.\n",
    "\n",
    "    :return: Nothing. Saves to CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the dataset.\n",
    "    slo_dataset_cmu = \\\n",
    "        pd.read_csv(\"borg-SLO-classifiers/dataset_20100101-20180510_tok.csv\", sep=\",\")\n",
    "\n",
    "    # Shuffle the data randomly.\n",
    "    slo_dataset_cmu = slo_dataset_cmu.reindex(\n",
    "        pd.np.random.permutation(slo_dataset_cmu.index))\n",
    "\n",
    "    # Generate a Pandas dataframe.\n",
    "    slo_dataframe_cmu = pd.DataFrame(slo_dataset_cmu)\n",
    "\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO CMU dataframe:\")\n",
    "    log.debug(slo_dataframe_cmu.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO CMU dataframe:\")\n",
    "    log.debug(slo_dataframe_cmu.head)\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    def preprocess_tweet_text(tweet_text):\n",
    "        \"\"\"\n",
    "        Helper function performs text pre-processing using regular expressions and other Python functions.\n",
    "\n",
    "        Notes:\n",
    "\n",
    "        Stop words are retained.\n",
    "\n",
    "        TODO - shrink character elongations\n",
    "        TODO - remove non-english tweets\n",
    "        TODO - remove non-company associated tweets\n",
    "        TODO - remove year and time.\n",
    "        TODO - remove cash items?\n",
    "\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Remove \"RT\" tags.\n",
    "        preprocessed_tweet_text = re.sub(\"rt\", \"\", tweet_text)\n",
    "\n",
    "        # Remove URL's.\n",
    "        preprocessed_tweet_text = re.sub(\"http[s]?://\\S+\", \"slo_url\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove Tweet mentions.\n",
    "        preprocessed_tweet_text = re.sub(\"@\\S+\", \"slo_mention\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove Tweet hashtags.\n",
    "        preprocessed_tweet_text = re.sub(\"#\\S+\", \"slo_hashtag\", preprocessed_tweet_text)\n",
    "\n",
    "        # Remove all punctuation.\n",
    "        preprocessed_tweet_text = preprocessed_tweet_text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        return preprocessed_tweet_text\n",
    "\n",
    "        # Assign new dataframe to contents of old.\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Down-case all text.\n",
    "    slo_dataframe_cmu['tweet_t'] = slo_dataframe_cmu['tweet_t'].str.lower()\n",
    "\n",
    "    # Pre-process each tweet individually.\n",
    "    for index in slo_dataframe_cmu.index:\n",
    "        slo_dataframe_cmu['tweet_t'][index] = preprocess_tweet_text(slo_dataframe_cmu['tweet_t'][index])\n",
    "\n",
    "    # Reindex everything.\n",
    "    slo_dataframe_cmu.index = pd.RangeIndex(len(slo_dataframe_cmu.index))\n",
    "    # slo_dataframe_combined.index = range(len(slo_dataframe_combined.index))\n",
    "\n",
    "    # Create input features.\n",
    "    selected_features_cmu = slo_dataframe_cmu['tweet_t']\n",
    "    processed_features_cmu = selected_features_cmu.copy()\n",
    "\n",
    "    # Check what we are using for predictions.\n",
    "    if debug:\n",
    "        log.debug(\"The shape of our SLO CMU feature dataframe:\")\n",
    "        log.debug(processed_features_cmu.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The columns of our SLO CMU feature dataframe:\")\n",
    "        log.debug(processed_features_cmu.head)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Save to CSV file.\n",
    "    slo_dataframe_cmu.to_csv(\"preprocessed-datasets/dataset_20100101-20180510_tok_PROCESSED.csv\", sep=',',\n",
    "                             encoding='utf-8', index=False)\n",
    "\n",
    "    # return processed_features_cmu\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "############################################################################################\n",
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\n",
    "Note: Used to individually test that the preprocessors function as intended.\n",
    "\"\"\"\n",
    "\n",
    "# Debug variable.\n",
    "debug_main = 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    Comment or uncomment in order to run the associated tweet preprocessor module.\n",
    "    \"\"\"\n",
    "    # tweet_dataset_preprocessor_1()\n",
    "    # tweet_dataset_preprocessor_2()\n",
    "    # tweet_dataset_preprocessor_3()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"Time taken to run pre-processor function:\")\n",
    "        time_taken = end_time - start_time\n",
    "        log.debug(time_taken)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "############################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## slo_topic_classification_clean.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "This Python program compares the accuracy metrics obtained by training various Scikit-Learn non-NN Classifiers and one NN Classifier on a small labeled TBL topic classification dataset.  It also makes TBL topic predictions on a large 600k+ preprocessed Twitter dataset for each trained Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Multinomial Naive Bayes Classifier:\n",
      "DEBUG:root:Mean accuracy over 1000 iterations is: 0.49251086956521645\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Prediction statistics using Multinomial Naive Bayes Classifier:\n",
      "DEBUG:root:The number of Tweets identified as social is :648413\n",
      "DEBUG:root:The % of Tweets identified as social in the entire dataset is: 98.3961625658971\n",
      "DEBUG:root:The number of Tweets identified as economic is :10569\n",
      "DEBUG:root:The % of Tweets identified as economic in the entire dataset is: 1.6038374341029042\n",
      "DEBUG:root:The number of Tweets identified as environmental is :0\n",
      "DEBUG:root:The % of Tweets identified as environmental in the entire dataset is: 0.0\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Stochastic Gradient Descent Classifier:\n",
      "DEBUG:root:Mean accuracy over 1000 iterations is: 0.49178260869565205\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Prediction statistics using Stochastic Gradient Descent Classifier:\n",
      "DEBUG:root:The number of Tweets identified as social is :658982\n",
      "DEBUG:root:The % of Tweets identified as social in the entire dataset is: 100.0\n",
      "DEBUG:root:The number of Tweets identified as economic is :0\n",
      "DEBUG:root:The % of Tweets identified as economic in the entire dataset is: 0.0\n",
      "DEBUG:root:The number of Tweets identified as environmental is :0\n",
      "DEBUG:root:The % of Tweets identified as environmental in the entire dataset is: 0.0\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Support Vector Classification Classifier:\n",
      "DEBUG:root:Mean accuracy over 1000 iterations is: 0.5349347826086978\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Prediction statistics using Support Vector Classification Classifier:\n",
      "DEBUG:root:The number of Tweets identified as social is :584878\n",
      "DEBUG:root:The % of Tweets identified as social in the entire dataset is: 88.75477630648486\n",
      "DEBUG:root:The number of Tweets identified as economic is :72514\n",
      "DEBUG:root:The % of Tweets identified as economic in the entire dataset is: 11.003942444558424\n",
      "DEBUG:root:The number of Tweets identified as environmental is :1590\n",
      "DEBUG:root:The % of Tweets identified as environmental in the entire dataset is: 0.24128124895672418\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Linear Support Vector Classification Classifier:\n",
      "DEBUG:root:Mean accuracy over 1000 iterations is: 0.5157717391304358\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Prediction statistics using Linear Support Vector Classification Classifier:\n",
      "DEBUG:root:The number of Tweets identified as social is :614761\n",
      "DEBUG:root:The % of Tweets identified as social in the entire dataset is: 93.28949804395265\n",
      "DEBUG:root:The number of Tweets identified as economic is :44109\n",
      "DEBUG:root:The % of Tweets identified as economic in the entire dataset is: 6.693506044171162\n",
      "DEBUG:root:The number of Tweets identified as environmental is :112\n",
      "DEBUG:root:The % of Tweets identified as environmental in the entire dataset is: 0.016995911876196923\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:KNeighbor Classifier:\n",
      "DEBUG:root:Mean accuracy over 1000 iterations is: 0.5179347826086967\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Prediction statistics using KNeighbor Classifier:\n",
      "DEBUG:root:The number of Tweets identified as social is :620407\n",
      "DEBUG:root:The % of Tweets identified as social in the entire dataset is: 94.14627410156878\n",
      "DEBUG:root:The number of Tweets identified as economic is :36004\n",
      "DEBUG:root:The % of Tweets identified as economic in the entire dataset is: 5.463578671344589\n",
      "DEBUG:root:The number of Tweets identified as environmental is :2571\n",
      "DEBUG:root:The % of Tweets identified as environmental in the entire dataset is: 0.3901472270866276\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Decision Tree Classifier:\n",
      "DEBUG:root:Mean accuracy over 1000 iterations is: 0.5322934782608705\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Prediction statistics using Decision Tree Classifier:\n",
      "DEBUG:root:The number of Tweets identified as social is :263958\n",
      "DEBUG:root:The % of Tweets identified as social in the entire dataset is: 40.05541881265346\n",
      "DEBUG:root:The number of Tweets identified as economic is :40518\n",
      "DEBUG:root:The % of Tweets identified as economic in the entire dataset is: 6.148574619640597\n",
      "DEBUG:root:The number of Tweets identified as environmental is :354506\n",
      "DEBUG:root:The % of Tweets identified as environmental in the entire dataset is: 53.796006567705945\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Multi Layer Perceptron Neural Network Classifier:\n",
      "DEBUG:root:Mean accuracy over 1000 iterations is: 0.4907282608695649\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Prediction statistics using Multi Layer Perceptron Neural Network Classifier:\n",
      "DEBUG:root:The number of Tweets identified as social is :658982\n",
      "DEBUG:root:The % of Tweets identified as social in the entire dataset is: 100.0\n",
      "DEBUG:root:The number of Tweets identified as economic is :0\n",
      "DEBUG:root:The % of Tweets identified as economic in the entire dataset is: 0.0\n",
      "DEBUG:root:The number of Tweets identified as environmental is :0\n",
      "DEBUG:root:The % of Tweets identified as environmental in the entire dataset is: 0.0\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Logistic Regression Classifier:\n",
      "DEBUG:root:Mean accuracy over 1000 iterations is: 0.5038586956521747\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:root:Prediction statistics using Logistic Regression Classifier:\n",
      "DEBUG:root:The number of Tweets identified as social is :574373\n",
      "DEBUG:root:The % of Tweets identified as social in the entire dataset is: 87.16065082202549\n",
      "DEBUG:root:The number of Tweets identified as economic is :74984\n",
      "DEBUG:root:The % of Tweets identified as economic in the entire dataset is: 11.378763001113839\n",
      "DEBUG:root:The number of Tweets identified as environmental is :9625\n",
      "DEBUG:root:The % of Tweets identified as environmental in the entire dataset is: 1.460586176860673\n",
      "DEBUG:root:\n",
      "\n",
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Course: CS 344 - Artificial Intelligence\n",
    "Instructor: Professor VanderLinden\n",
    "Name: Joseph Jinn\n",
    "Date: 4-23-19\n",
    "\n",
    "Final Project - SLO TBL Topic Classification\n",
    "\n",
    "###########################################################\n",
    "Notes:\n",
    "\n",
    "Utilizes Scikit-Learn machine learning algorithms for fast prototyping and topic classification using a variety\n",
    "of Classifiers.\n",
    "\n",
    "TODO - resolve SettingWithCopyWarning.\n",
    "\n",
    "TODO - implement data visualizations via matplotlib and Seaborn.\n",
    "\n",
    "TODO - attempt to acquire additional labeled Tweets for topic classification using pattern matching and pandas queries.\n",
    "TODO - reference settings.py and autocoding.py for template of how to do this.\n",
    "\n",
    "TODO - revise report.ipynb and paper as updates are made to implementation and code-base.\n",
    "\n",
    "###########################################################\n",
    "Resources Used:\n",
    "\n",
    "Refer to original un-cleaned version.\n",
    "\n",
    "https://scikit-plot.readthedocs.io/en/stable/index.html\n",
    "(visualizations simplified)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "import logging as log\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Note: FIXME - indicates unresolved import error, but still runs fine.\n",
    "# noinspection PyUnresolvedReferences\n",
    "from SLO_TBL_Tweet_Preprocessor_Specialized import tweet_dataset_preprocessor_1, tweet_dataset_preprocessor_2, \\\n",
    "    tweet_dataset_preprocessor_3\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# Note: Need to set level AND turn on debug variables in order to see all debug output.\n",
    "log.basicConfig(level=log.DEBUG)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Miscellaneous parameter adjustments for pandas and python.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# Turn on and off to debug various sub-sections.\n",
    "debug = False\n",
    "debug_pipeline = False\n",
    "debug_preprocess_tweets = False\n",
    "debug_train_test_set_creation = False\n",
    "debug_classifier_iterations = False\n",
    "debug_create_prediction_set = False\n",
    "debug_make_predictions = False\n",
    "\n",
    "################################################################################################################\n",
    "################################################################################################################\n",
    "\n",
    "# Import the datasets.\n",
    "tweet_dataset_processed1 = \\\n",
    "    pd.read_csv(\"preprocessed-datasets/tbl_kvlinden_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "tweet_dataset_processed2 = \\\n",
    "    pd.read_csv(\"preprocessed-datasets/tbl_training_set_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "# Reindex and shuffle the data randomly.\n",
    "tweet_dataset_processed1 = tweet_dataset_processed1.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed1.index))\n",
    "\n",
    "tweet_dataset_processed2 = tweet_dataset_processed2.reindex(\n",
    "    pd.np.random.permutation(tweet_dataset_processed2.index))\n",
    "\n",
    "# Generate a Pandas dataframe.\n",
    "tweet_dataframe_processed1 = pd.DataFrame(tweet_dataset_processed1)\n",
    "tweet_dataframe_processed2 = pd.DataFrame(tweet_dataset_processed2)\n",
    "\n",
    "if debug_preprocess_tweets:\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO dataframe 1:\")\n",
    "    log.debug(tweet_dataframe_processed1.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO dataframe 1:\")\n",
    "    log.debug(tweet_dataframe_processed1.head)\n",
    "    log.debug(\"\\n\")\n",
    "    # Print shape and column names.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The shape of our SLO dataframe 2:\")\n",
    "    log.debug(tweet_dataframe_processed2.shape)\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The columns of our SLO dataframe 2:\")\n",
    "    log.debug(tweet_dataframe_processed2.head)\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "# Concatenate the individual datasets together.\n",
    "frames = [tweet_dataframe_processed1, tweet_dataframe_processed2]\n",
    "slo_dataframe_combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Reindex everything.\n",
    "slo_dataframe_combined.index = pd.RangeIndex(len(slo_dataframe_combined.index))\n",
    "# slo_dataframe_combined.index = range(len(slo_dataframe_combined.index))\n",
    "\n",
    "# Assign column names.\n",
    "tweet_dataframe_processed_column_names = ['Tweet', 'SLO']\n",
    "\n",
    "# Create input features.\n",
    "selected_features = slo_dataframe_combined[tweet_dataframe_processed_column_names]\n",
    "processed_features = selected_features.copy()\n",
    "\n",
    "if debug_preprocess_tweets:\n",
    "    # Check what we are using as inputs.\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"The Tweets in our input feature:\")\n",
    "    log.debug(processed_features['Tweet'])\n",
    "    log.debug(\"\\n\")\n",
    "    log.debug(\"SLO TBL topic classification label for each Tweet:\")\n",
    "    log.debug(processed_features['SLO'])\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "# Create feature set and target sets.\n",
    "slo_feature_set = processed_features['Tweet']\n",
    "slo_target_set = processed_features['SLO']\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "def create_training_and_test_set():\n",
    "    \"\"\"\n",
    "    This functions splits the feature and target set into training and test sets for each set.\n",
    "\n",
    "    Note: We use this to generate a randomized training and target set in order to average our results over\n",
    "    n iterations.\n",
    "\n",
    "    random_state = rng (where rng = random number seed generator)\n",
    "\n",
    "    :return: Nothing.  Global variables are established.\n",
    "    \"\"\"\n",
    "    global tweet_train, tweet_test, target_train, target_test, target_train_encoded, target_test_encoded\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import random\n",
    "    rng = random.randint(1, 1000000)\n",
    "    # Split feature and target set into training and test sets for each set.\n",
    "    tweet_train, tweet_test, target_train, target_test = train_test_split(slo_feature_set, slo_target_set,\n",
    "                                                                          test_size=0.33,\n",
    "                                                                          random_state=rng)\n",
    "\n",
    "    if debug_train_test_set_creation:\n",
    "        log.debug(\"Shape of tweet training set:\")\n",
    "        log.debug(tweet_train.data.shape)\n",
    "        log.debug(\"Shape of tweet test set:\")\n",
    "        log.debug(tweet_test.data.shape)\n",
    "        log.debug(\"Shape of target training set:\")\n",
    "        log.debug(target_train.data.shape)\n",
    "        log.debug(\"Shape of target test set:\")\n",
    "        log.debug(target_test.data.shape)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Use Sci-kit learn to encode labels into integer values - one assigned integer value per class.\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    target_label_encoder = preprocessing.LabelEncoder()\n",
    "    target_train_encoded = target_label_encoder.fit_transform(target_train)\n",
    "    target_test_encoded = target_label_encoder.fit_transform(target_test)\n",
    "\n",
    "    target_train_decoded = target_label_encoder.inverse_transform(target_train_encoded)\n",
    "    target_test_decoded = target_label_encoder.inverse_transform(target_test_encoded)\n",
    "\n",
    "    if debug_train_test_set_creation:\n",
    "        log.debug(\"Encoded target training labels:\")\n",
    "        log.debug(target_train_encoded)\n",
    "        log.debug(\"Decoded target training labels:\")\n",
    "        log.debug(target_train_decoded)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"Encoded target test labels:\")\n",
    "        log.debug(target_test_encoded)\n",
    "        log.debug(\"Decoded target test labels:\")\n",
    "        log.debug(target_test_decoded)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # return [tweet_train, tweet_test, target_train, target_test, target_train_encoded, target_test_encoded]\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "def scikit_learn_multinomialnb_classifier_non_pipeline():\n",
    "    \"\"\"\n",
    "    Function trains a Multinomial Naive Bayes Classifier without using a Pipeline.\n",
    "\n",
    "    Note: Implemented for educational purposes - so I can see the manual workflow, otherwise the Pipeline Class hides\n",
    "    these details and we only have to tune parameters.\n",
    "\n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the training and test sets from the feature and target sets.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    # Use Sci-kit learn to tokenize each Tweet and convert into a bag-of-words sparse feature vector.\n",
    "    vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "    tweet_train_encoded = vectorizer.fit_transform(tweet_train)\n",
    "    tweet_test_encoded = vectorizer.transform(tweet_test)\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"Vectorized tweet training set:\")\n",
    "        log.debug(tweet_train_encoded)\n",
    "        log.debug(\"Vectorized tweet testing set:\")\n",
    "        log.debug(tweet_test_encoded)\n",
    "        log.debug(\"Shape of the tweet training set:\")\n",
    "        log.debug(tweet_train_encoded.shape)\n",
    "        log.debug(\"Shape of the tweet testing set:\")\n",
    "        log.debug(tweet_test_encoded.shape)\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    # Use Sci-kit learn to convert each tokenized Tweet into term frequencies.\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    tweet_train_encoded_tfidf = tfidf_transformer.fit_transform(tweet_train_encoded)\n",
    "    tweet_test_encoded_tfidf = tfidf_transformer.transform(tweet_test_encoded)\n",
    "\n",
    "    if debug:\n",
    "        log.debug(\"vectorized tweet training set term frequencies down-sampled:\")\n",
    "        log.debug(tweet_train_encoded_tfidf)\n",
    "        log.debug(\"Shape of the tweet training set term frequencies down-sampled: \")\n",
    "        log.debug(tweet_train_encoded_tfidf.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"vectorized tweet test set term frequencies down-sampled:\")\n",
    "        log.debug(tweet_test_encoded_tfidf)\n",
    "        log.debug(\"Shape of the tweet test set term frequencies down-sampled: \")\n",
    "        log.debug(tweet_test_encoded_tfidf.shape)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    #######################################################\n",
    "\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    # Train the Multinomial Naive Bayes Classifier.\n",
    "    clf_multinomial_nb = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "    clf_multinomial_nb.fit(tweet_train_encoded_tfidf, target_train_encoded)\n",
    "\n",
    "    # Predict using the Multinomial Naive Bayes Classifier.\n",
    "    clf_multinomial_nb_predict = clf_multinomial_nb.predict(tweet_test_encoded_tfidf)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    log.debug(\"MultinomialNB Classifier accuracy using accuracy_score() function : \",\n",
    "              accuracy_score(target_test_encoded, clf_multinomial_nb_predict, normalize=True))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Another method of obtaining accuracy metric.\n",
    "    log.debug(\"Accuracy for test set predictions using multinomialNB:\")\n",
    "    log.debug(str(np.mean(clf_multinomial_nb_predict == target_test_encoded)))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # View the results as Tweet => predicted topic classification label.\n",
    "    for doc, category in zip(tweet_test, clf_multinomial_nb_predict):\n",
    "        log.debug('%r => %s' % (doc, category))\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def create_prediction_set():\n",
    "    \"\"\"\n",
    "    Function prepares the borg-classifier dataset to be used for predictions in trained models.\n",
    "\n",
    "    :return: the prepared dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the dataset.\n",
    "    slo_dataset_cmu = \\\n",
    "        pd.read_csv(\"preprocessed-datasets/dataset_20100101-20180510_tok_PROCESSED.csv\", sep=\",\")\n",
    "\n",
    "    # Shuffle the data randomly.\n",
    "    slo_dataset_cmu = slo_dataset_cmu.reindex(\n",
    "        pd.np.random.permutation(slo_dataset_cmu.index))\n",
    "\n",
    "    # Generate a Pandas dataframe.\n",
    "    slo_dataframe_cmu = pd.DataFrame(slo_dataset_cmu)\n",
    "\n",
    "    if debug_create_prediction_set:\n",
    "        # Print shape and column names.\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The shape of our SLO CMU dataframe:\")\n",
    "        log.debug(slo_dataframe_cmu.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The columns of our SLO CMU dataframe:\")\n",
    "        log.debug(slo_dataframe_cmu.head)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Reindex everything.\n",
    "    slo_dataframe_cmu.index = pd.RangeIndex(len(slo_dataframe_cmu.index))\n",
    "    # slo_dataframe_cmu.index = range(len(slo_dataframe_cmu.index))\n",
    "\n",
    "    # Create input features.\n",
    "    # Note: using \"filter()\" - other methods seems to result in shape of (658982, ) instead of (658982, 1)\n",
    "    selected_features_cmu = slo_dataframe_cmu.filter(['tweet_t'])\n",
    "    processed_features_cmu = selected_features_cmu.copy()\n",
    "\n",
    "    # Rename column.\n",
    "    processed_features_cmu.columns = ['Tweets']\n",
    "\n",
    "    if debug_create_prediction_set:\n",
    "        # Print shape and column names.\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The shape of our processed features:\")\n",
    "        log.debug(processed_features_cmu.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The columns of our processed features:\")\n",
    "        log.debug(processed_features_cmu.head)\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    if debug_create_prediction_set:\n",
    "        # Check what we are using as inputs.\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The Tweets in our input feature:\")\n",
    "        log.debug(processed_features_cmu['Tweets'])\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    return processed_features_cmu\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def make_predictions(trained_model):\n",
    "    \"\"\"\n",
    "    Function makes predictions using the trained model passed as an argument.\n",
    "\n",
    "    :param trained_model\n",
    "    :return: Nothingl.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the dataset to be used for predictions.\n",
    "    prediction_set = create_prediction_set()\n",
    "\n",
    "    # Make predictions of the borg-slo-classifiers dataset.\n",
    "    # Note to self: don't be an idiot and try to make predictions on the entire dataframe object instead of a column.\n",
    "    predictions = trained_model.predict(prediction_set['Tweets'])\n",
    "\n",
    "    # Store predictions in Pandas dataframe.\n",
    "    results_df = pd.DataFrame(predictions)\n",
    "\n",
    "    # Assign column names.\n",
    "    results_df_column_name = ['TBL_classification']\n",
    "    results_df.columns = results_df_column_name\n",
    "\n",
    "    if debug_make_predictions:\n",
    "        log.debug(\"The shape of our prediction results dataframe:\")\n",
    "        log.debug(results_df.shape)\n",
    "        log.debug(\"\\n\")\n",
    "        log.debug(\"The contents of our prediction results dataframe:\")\n",
    "        log.debug(results_df.head())\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # Count # of each classifications made.\n",
    "    social_counter = 0\n",
    "    economic_counter = 0\n",
    "    environmental_counter = 0\n",
    "\n",
    "    for index in results_df.index:\n",
    "        if results_df['TBL_classification'][index] == 'economic':\n",
    "            economic_counter += 1\n",
    "        if results_df['TBL_classification'][index] == 'social':\n",
    "            social_counter += 1\n",
    "        if results_df['TBL_classification'][index] == 'environmental':\n",
    "            environmental_counter += 1\n",
    "\n",
    "    # Calculate percentages for each classification.\n",
    "    social_percentage = (social_counter / results_df.shape[0]) * 100.0\n",
    "    economic_percentage = (economic_counter / results_df.shape[0]) * 100.0\n",
    "    environmental_percentage = (environmental_counter / results_df.shape[0]) * 100.0\n",
    "\n",
    "    # Display our statistics.\n",
    "    log.debug(\"The number of Tweets identified as social is :\" + str(social_counter))\n",
    "    log.debug(\"The % of Tweets identified as social in the entire dataset is: \" + str(social_percentage))\n",
    "    log.debug(\"The number of Tweets identified as economic is :\" + str(economic_counter))\n",
    "    log.debug(\"The % of Tweets identified as economic in the entire dataset is: \" + str(economic_percentage))\n",
    "    log.debug(\"The number of Tweets identified as environmental is :\" + str(environmental_counter))\n",
    "    log.debug(\"The % of Tweets identified as environmental in the entire dataset is: \" + str(environmental_percentage))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def multinomial_naive_bayes_classifier_grid_search():\n",
    "    \"\"\"\n",
    "    Function performs a exhaustive grid search to find the best hyper-parameters for use training the model.\n",
    "\n",
    "    :return: Nothing.\n",
    "    \"\"\"\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    multinomial_nb_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.10],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search using all cores.\n",
    "    multinomial_nb_clf = GridSearchCV(multinomial_nb_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "    # Train and predict on optimal parameters found by Grid Search.\n",
    "    multinomial_nb_clf.fit(tweet_train, target_train)\n",
    "    multinomial_nb_predictions = multinomial_nb_clf.predict(tweet_test)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(multinomial_nb_clf.cv_results_)\n",
    "        log.debug(\"The shape of the Multinomial Naive Bayes Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\"The contents of the Multinomial Naive Bayes Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the Multinomial Naive Bayes Classifier is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, multinomial_nb_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Display the accuracy we obtained using the optimal parameters.\n",
    "    log.debug(\"Accuracy using Multinomial Naive Bayes Classifier Grid Search is: \")\n",
    "    log.debug(np.mean(multinomial_nb_predictions == target_test))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def multinomial_naive_bayes_classifier():\n",
    "    \"\"\"\n",
    "    Functions trains a Multinomial Naive Bayes Classifier.\n",
    "\n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    multinomial_nb_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)),\n",
    "    ])\n",
    "\n",
    "    # Predict n iterations and calculate mean accuracy.\n",
    "    mean_accuracy = 0.0\n",
    "    iterations = 1000\n",
    "    for index in range(0, iterations):\n",
    "\n",
    "        # Create randomized training and test set using our dataset.\n",
    "        create_training_and_test_set()\n",
    "\n",
    "        multinomial_nb_clf.fit(tweet_train, target_train)\n",
    "        multinomial_nb_predictions = multinomial_nb_clf.predict(tweet_test)\n",
    "\n",
    "        # Calculate the accuracy of our predictions.\n",
    "        accuracy = np.mean(multinomial_nb_predictions == target_test)\n",
    "\n",
    "        if debug_classifier_iterations:\n",
    "            # Measure accuracy.\n",
    "            log.debug(\"\\n\")\n",
    "            log.debug(\"Accuracy for test set predictions using Multinomial Naive Bayes Classifier:\")\n",
    "            log.debug(str(accuracy))\n",
    "            log.debug(\"\\n\")\n",
    "\n",
    "            log.debug(\"Multinomial Naive Bayes Classifier Metrics\")\n",
    "            log.debug(metrics.classification_report(target_test, multinomial_nb_predictions,\n",
    "                                                    target_names=['economic', 'environmental', 'social']))\n",
    "\n",
    "            log.debug(\"Multinomial Naive Bayes Classifier confusion matrix:\")\n",
    "            log.debug(metrics.confusion_matrix(target_test, multinomial_nb_predictions))\n",
    "\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy = mean_accuracy / iterations\n",
    "    log.debug(\"Multinomial Naive Bayes Classifier:\")\n",
    "    log.debug(\"Mean accuracy over \" + str(iterations) + \" iterations is: \" + str(mean_accuracy))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Make predictions using trained model.\n",
    "    log.debug(\"Prediction statistics using Multinomial Naive Bayes Classifier:\")\n",
    "    make_predictions(multinomial_nb_clf)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "def sgd_classifier_grid_search():\n",
    "    \"\"\"\n",
    "    Function performs a exhaustive grid search to find the best hyper-parameters for use training the model.\n",
    "\n",
    "    :return: Nothing.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    sgd_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "                              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "                              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
    "                              n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
    "                              power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
    "                              validation_fraction=0.1, verbose=0, warm_start=False)),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__alpha': (1e-1, 1e-2, 1e-3, 0.00001, 0.000001),\n",
    "    }\n",
    "\n",
    "    # Perform the grid search using all cores.\n",
    "    sgd_classifier_clf = GridSearchCV(sgd_classifier_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "    # Train and predict on optimal parameters found by Grid Search.\n",
    "    sgd_classifier_clf.fit(tweet_train, target_train)\n",
    "    sgd_classifier_predictions = sgd_classifier_clf.predict(tweet_test)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(sgd_classifier_clf.cv_results_)\n",
    "        log.debug(\"The shape of the SGD Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\"The contents of the SGD Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the SGD Classifier is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, sgd_classifier_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Display the accuracy we obtained using the optimal parameters.\n",
    "    log.debug(\"Accuracy using Stochastic Gradient Descent Classifier Grid Search is: \")\n",
    "    log.debug(np.mean(sgd_classifier_predictions == target_test))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def sgd_classifier():\n",
    "    \"\"\"\n",
    "    Function trains a Stochastic Gradient Descent Classifier.\n",
    "    \n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "    sgd_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "        ('clf', SGDClassifier(alpha=0.1, average=False, class_weight=None,\n",
    "                              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "                              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=5,\n",
    "                              n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
    "                              power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
    "                              validation_fraction=0.1, verbose=0, warm_start=False)),\n",
    "    ])\n",
    "\n",
    "    # Predict n iterations and calculate mean accuracy.\n",
    "    mean_accuracy = 0.0\n",
    "    iterations = 1000\n",
    "    for index in range(0, iterations):\n",
    "\n",
    "        # Create randomized training and test set using our dataset.\n",
    "        create_training_and_test_set()\n",
    "\n",
    "        sgd_classifier_clf.fit(tweet_train, target_train)\n",
    "        sgd_classifier_predictions = sgd_classifier_clf.predict(tweet_test)\n",
    "\n",
    "        # Calculate the accuracy of our predictions.\n",
    "        accuracy = np.mean(sgd_classifier_predictions == target_test)\n",
    "\n",
    "        if debug_classifier_iterations:\n",
    "            # Measure accuracy.\n",
    "            log.debug(\"\\n\")\n",
    "            log.debug(\"Accuracy for test set predictions using Stochastic Gradient Descent Classifier:\")\n",
    "            log.debug(str(accuracy))\n",
    "            log.debug(\"\\n\")\n",
    "\n",
    "            log.debug(\"SGD_classifier Classifier Metrics\")\n",
    "            log.debug(metrics.classification_report(target_test, sgd_classifier_predictions,\n",
    "                                                    target_names=['economic', 'environmental', 'social']))\n",
    "\n",
    "            log.debug(\"SGD_classifier confusion matrix:\")\n",
    "            log.debug(metrics.confusion_matrix(target_test, sgd_classifier_predictions))\n",
    "\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy = mean_accuracy / iterations\n",
    "    log.debug(\"Stochastic Gradient Descent Classifier:\")\n",
    "    log.debug(\"Mean accuracy over \" + str(iterations) + \" iterations is: \" + str(mean_accuracy))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Make predictions using trained model.\n",
    "    log.debug(\"Prediction statistics using Stochastic Gradient Descent Classifier:\")\n",
    "    make_predictions(sgd_classifier_clf)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def svm_support_vector_classification_grid_search():\n",
    "    \"\"\"\n",
    "    Function performs a exhaustive grid search to find the best hyper-parameters for use training the model.\n",
    "\n",
    "    :return: Nothing.\n",
    "    \"\"\"\n",
    "    from sklearn import svm\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    svc_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                        decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "                        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "                        tol=0.001, verbose=False)),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'clf__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "        'clf__gamma': ['scale', 'auto'],\n",
    "        'clf__shrinking': (True, False),\n",
    "        'clf__probability': (True, False),\n",
    "        'clf__tol': [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "        'clf__decision_function_shape': ['ovo', 'ovr'],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search using all cores.\n",
    "    svc_classifier_clf = GridSearchCV(svc_classifier_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "    # Train and predict on optimal parameters found by Grid Search.\n",
    "    svc_classifier_clf.fit(tweet_train, target_train)\n",
    "    svc_classifier_predictions = svc_classifier_clf.predict(tweet_test)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(svc_classifier_clf.cv_results_)\n",
    "        log.debug(\"The shape of the Support Vector Classification Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\"The contents of the Support Vector Classification Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the Support Vector Classification Classifier is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, svc_classifier_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Display the accuracy we obtained using the optimal parameters.\n",
    "    log.debug(\"Accuracy using Support Vector Classification Classifier Grid Search is: \")\n",
    "    log.debug(np.mean(svc_classifier_predictions == target_test))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def svm_support_vector_classification():\n",
    "    \"\"\"\n",
    "    Functions trains a Support Vector Machine - Support Vector Classification Classifier.\n",
    "    \n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "    from sklearn import svm\n",
    "\n",
    "    svc_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', svm.SVC(C=0.9, cache_size=200, class_weight=None, coef0=0.0,\n",
    "                        decision_function_shape='ovo', degree=3, gamma='scale', kernel='sigmoid',\n",
    "                        max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "                        tol=0.01, verbose=False)),\n",
    "    ])\n",
    "\n",
    "    # Predict n iterations and calculate mean accuracy.\n",
    "    mean_accuracy = 0.0\n",
    "    iterations = 1000\n",
    "    for index in range(0, iterations):\n",
    "        # Create randomized training and test set using our dataset.\n",
    "        create_training_and_test_set()\n",
    "\n",
    "        svc_classifier_clf.fit(tweet_train, target_train)\n",
    "        svc_classifier_predictions = svc_classifier_clf.predict(tweet_test)\n",
    "\n",
    "        # Calculate the accuracy of our predictions.\n",
    "        accuracy = np.mean(svc_classifier_predictions == target_test)\n",
    "\n",
    "        if debug_classifier_iterations:\n",
    "            # Measure accuracy.\n",
    "            log.debug(\"\\n\")\n",
    "            log.debug(\"Accuracy for test set predictions using Support Vector Classification Classifier:\")\n",
    "            log.debug(str(accuracy))\n",
    "            log.debug(\"\\n\")\n",
    "\n",
    "            log.debug(\"SVC_classifier Metrics\")\n",
    "            log.debug(metrics.classification_report(target_test, svc_classifier_predictions,\n",
    "                                                    target_names=['economic', 'environmental', 'social']))\n",
    "\n",
    "            log.debug(\"SVC_classifier confusion matrix:\")\n",
    "            log.debug(metrics.confusion_matrix(target_test, svc_classifier_predictions))\n",
    "\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy = mean_accuracy / iterations\n",
    "    log.debug(\"Support Vector Classification Classifier:\")\n",
    "    log.debug(\"Mean accuracy over \" + str(iterations) + \" iterations is: \" + str(mean_accuracy))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Make predictions using trained model.\n",
    "    log.debug(\"Prediction statistics using Support Vector Classification Classifier:\")\n",
    "    make_predictions(svc_classifier_clf)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def svm_linear_support_vector_classification_grid_search():\n",
    "    \"\"\"\n",
    "    Function performs a exhaustive grid search to find the best hyper-parameters for use training the model.\n",
    "\n",
    "    :return: Nothing.\n",
    "    \"\"\"\n",
    "    from sklearn import svm\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    linear_svc_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', svm.LinearSVC(C=0.7, class_weight=None, dual=True, fit_intercept=True,\n",
    "                              intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "                              multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "                              verbose=0)),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__loss': ['squared_hinge'],\n",
    "        # 'clf__dual': (True, False),\n",
    "        'clf__multi_class': ['ovr', 'crammer_singer'],\n",
    "        'clf__tol': [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "        'clf__fit_intercept': (True, False),\n",
    "        'clf__max_iter': [500, 1000, 1500, 2000],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search using all cores.\n",
    "    linear_svc_classifier_clf = GridSearchCV(linear_svc_classifier_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "    # Train and predict on optimal parameters found by Grid Search.\n",
    "    linear_svc_classifier_clf.fit(tweet_train, target_train)\n",
    "    linear_svc_classifier_predictions = linear_svc_classifier_clf.predict(tweet_test)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(linear_svc_classifier_clf.cv_results_)\n",
    "        log.debug(\"The shape of the Linear Support Vector Classification Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\n",
    "            \"The contents of the Linear Support Vector Classification Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the Linear Support Vector Classification Classifier is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, linear_svc_classifier_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Display the accuracy we obtained using the optimal parameters.\n",
    "    log.debug(\"Accuracy using Linear Support Vector Classification Classifier Grid Search is: \")\n",
    "    log.debug(np.mean(linear_svc_classifier_predictions == target_test))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def svm_linear_support_vector_classification():\n",
    "    \"\"\"\"\n",
    "    Function trains a Support Vector Machine - Linear Support Vector Classification Classifier.\n",
    "    \n",
    "    :return: none.\n",
    "    \"\"\"\n",
    "    from sklearn import svm\n",
    "\n",
    "    linear_svc_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', svm.LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
    "                              intercept_scaling=1, loss='squared_hinge', max_iter=2000,\n",
    "                              multi_class='ovr', penalty='l2', random_state=None, tol=0.1,\n",
    "                              verbose=0)),\n",
    "    ])\n",
    "\n",
    "    # Predict n iterations and calculate mean accuracy.\n",
    "    mean_accuracy = 0.0\n",
    "    iterations = 1000\n",
    "    for index in range(0, iterations):\n",
    "        # Create randomized training and test set using our dataset.\n",
    "        create_training_and_test_set()\n",
    "\n",
    "        linear_svc_classifier_clf.fit(tweet_train, target_train)\n",
    "        linear_svc_classifier_predictions = linear_svc_classifier_clf.predict(tweet_test)\n",
    "\n",
    "        # Calculate the accuracy of our predictions.\n",
    "        accuracy = np.mean(linear_svc_classifier_predictions == target_test)\n",
    "\n",
    "        if debug_classifier_iterations:\n",
    "            # Measure accuracy.\n",
    "            log.debug(\"\\n\")\n",
    "            log.debug(\"Accuracy for test set predictions using Linear Support Vector Classification Classifier:\")\n",
    "            log.debug(str(accuracy))\n",
    "            log.debug(\"\\n\")\n",
    "\n",
    "            log.debug(\"LinearSVC_classifier Metrics\")\n",
    "            log.debug(metrics.classification_report(target_test, linear_svc_classifier_predictions,\n",
    "                                                    target_names=['economic', 'environmental', 'social']))\n",
    "\n",
    "            log.debug(\"LinearSVC_classifier confusion matrix:\")\n",
    "            log.debug(metrics.confusion_matrix(target_test, linear_svc_classifier_predictions))\n",
    "\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy = mean_accuracy / iterations\n",
    "    log.debug(\"Linear Support Vector Classification Classifier:\")\n",
    "    log.debug(\"Mean accuracy over \" + str(iterations) + \" iterations is: \" + str(mean_accuracy))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Make predictions using trained model.\n",
    "    log.debug(\"Prediction statistics using Linear Support Vector Classification Classifier:\")\n",
    "    make_predictions(linear_svc_classifier_clf)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def nearest_kneighbor_classifier_grid_search():\n",
    "    \"\"\"\n",
    "       Function performs a exhaustive grid search to find the best hyper-parameters for use training the model.\n",
    "\n",
    "       :return: Nothing.\n",
    "       \"\"\"\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    k_neighbor_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=3, n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__n_neighbors': [10, 15, 20, 25, 30],\n",
    "        'clf__weights': ['uniform', 'distance'],\n",
    "        'clf__algorithm': ['auto'],\n",
    "        'clf__leaf_size': [5, 10, 15, 20],\n",
    "        'clf__p': [1, 2, 3, 4],\n",
    "        'clf__metric': ['euclidean', 'manhattan'],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search using all cores.\n",
    "    k_neighbor_classifier_clf = GridSearchCV(k_neighbor_classifier_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "    # Train and predict on optimal parameters found by Grid Search.\n",
    "    k_neighbor_classifier_clf.fit(tweet_train, target_train)\n",
    "    k_neighbor_classifier_predictions = k_neighbor_classifier_clf.predict(tweet_test)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(k_neighbor_classifier_clf.cv_results_)\n",
    "        log.debug(\"The shape of the KNeighbor Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\n",
    "            \"The contents of the  KNeighbor Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the KNeighbor Classifier is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, k_neighbor_classifier_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Display the accuracy we obtained using the optimal parameters.\n",
    "    log.debug(\"Accuracy using KNeighbor Classifier Grid Search is: \")\n",
    "    log.debug(np.mean(k_neighbor_classifier_predictions == target_test))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def nearest_kneighbor_classifier():\n",
    "    \"\"\"\n",
    "    Function trains a Nearest Neighbor - KNeighbor Classifier.\n",
    "    \n",
    "    :return: none. \n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "    k_neighbor_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=30, algorithm='auto', leaf_size=10, metric='euclidean', p=1,\n",
    "                                     weights='uniform', n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "    # Predict n iterations and calculate mean accuracy.\n",
    "    mean_accuracy = 0.0\n",
    "    iterations = 1000\n",
    "    for index in range(0, iterations):\n",
    "        # Create randomized training and test set using our dataset.\n",
    "        create_training_and_test_set()\n",
    "\n",
    "        k_neighbor_classifier_clf.fit(tweet_train, target_train)\n",
    "        k_neighbor_classifier_predictions = k_neighbor_classifier_clf.predict(tweet_test)\n",
    "\n",
    "        # Calculate the accuracy of our predictions.\n",
    "        accuracy = np.mean(k_neighbor_classifier_predictions == target_test)\n",
    "\n",
    "        if debug_classifier_iterations:\n",
    "            # Measure accuracy.\n",
    "            log.debug(\"\\n\")\n",
    "            log.debug(\"Accuracy for test set predictions using KNeighbor Classifier:\")\n",
    "            log.debug(str(accuracy))\n",
    "            log.debug(\"\\n\")\n",
    "\n",
    "            log.debug(\"KNeighbor_classifier Metrics\")\n",
    "            log.debug(metrics.classification_report(target_test, k_neighbor_classifier_predictions,\n",
    "                                                    target_names=['economic', 'environmental', 'social']))\n",
    "\n",
    "            log.debug(\"KNeighbor_classifier confusion matrix:\")\n",
    "            log.debug(metrics.confusion_matrix(target_test, k_neighbor_classifier_predictions))\n",
    "\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy = mean_accuracy / iterations\n",
    "    log.debug(\"KNeighbor Classifier:\")\n",
    "    log.debug(\"Mean accuracy over \" + str(iterations) + \" iterations is: \" + str(mean_accuracy))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Make predictions using trained model.\n",
    "    log.debug(\"Prediction statistics using KNeighbor Classifier:\")\n",
    "    make_predictions(k_neighbor_classifier_clf)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def decision_tree_classifier_grid_search():\n",
    "    \"\"\"\n",
    "       Function performs a exhaustive grid search to find the best hyper-parameters for use training the model.\n",
    "\n",
    "       :return: Nothing.\n",
    "       \"\"\"\n",
    "    from sklearn import tree\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    decision_tree_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', tree.DecisionTreeClassifier()),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__criterion': ['gini', 'entropy'],\n",
    "        'clf__max_depth': [None],\n",
    "        'clf__min_samples_split': [2, 3, 4],\n",
    "        'clf__min_samples_leaf': [1, 2, 3, 4],\n",
    "        'clf__min_weight_fraction_leaf': [0],\n",
    "        'clf__max_features': [None, 'sqrt', 'log2'],\n",
    "        'clf__max_leaf_nodes': [None, 2, 3, 4],\n",
    "        'clf__min_impurity_decrease': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search using all cores.\n",
    "    decision_tree_classifier_clf = GridSearchCV(decision_tree_classifier_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "    # Train and predict on optimal parameters found by Grid Search.\n",
    "    decision_tree_classifier_clf.fit(tweet_train, target_train)\n",
    "    decision_tree_classifier_predictions = decision_tree_classifier_clf.predict(tweet_test)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(decision_tree_classifier_clf.cv_results_)\n",
    "        log.debug(\"The shape of the Decision Tree Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\n",
    "            \"The contents of the Decision Tree Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the Decision Tree Classifier is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, decision_tree_classifier_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Display the accuracy we obtained using the optimal parameters.\n",
    "    log.debug(\"Accuracy using Decision Tree Classifier Grid Search is: \")\n",
    "    log.debug(np.mean(decision_tree_classifier_predictions == target_test))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def decision_tree_classifier():\n",
    "    \"\"\"\n",
    "    Functions trains a Decision Tree Classifier.\n",
    "    \n",
    "    :return: none. \n",
    "    \"\"\"\n",
    "    from sklearn import tree\n",
    "\n",
    "    decision_tree_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', tree.DecisionTreeClassifier(criterion='gini', max_depth=None, max_features=None,\n",
    "                                            max_leaf_nodes=3, min_impurity_decrease=1e-5, min_samples_leaf=1,\n",
    "                                            min_samples_split=2, min_weight_fraction_leaf=0)),\n",
    "    ])\n",
    "\n",
    "    # Predict n iterations and calculate mean accuracy.\n",
    "    mean_accuracy = 0.0\n",
    "    iterations = 1000\n",
    "    for index in range(0, iterations):\n",
    "        # Create randomized training and test set using our dataset.\n",
    "        create_training_and_test_set()\n",
    "\n",
    "        decision_tree_classifier_clf.fit(tweet_train, target_train)\n",
    "        decision_tree_classifier_predictions = decision_tree_classifier_clf.predict(tweet_test)\n",
    "\n",
    "        # Calculate the accuracy of our predictions.\n",
    "        accuracy = np.mean(decision_tree_classifier_predictions == target_test)\n",
    "\n",
    "        if debug_classifier_iterations:\n",
    "            # Measure accuracy.\n",
    "            log.debug(\"\\n\")\n",
    "            log.debug(\"Accuracy for test set predictions using Decision Tree Classifier:\")\n",
    "            log.debug(str(accuracy))\n",
    "            log.debug(\"\\n\")\n",
    "\n",
    "            log.debug(\"DecisionTree_classifier Metrics\")\n",
    "            log.debug(metrics.classification_report(target_test, decision_tree_classifier_predictions,\n",
    "                                                    target_names=['economic', 'environmental', 'social']))\n",
    "\n",
    "            log.debug(\"DecisionTree_classifier confusion matrix:\")\n",
    "            log.debug(metrics.confusion_matrix(target_test, decision_tree_classifier_predictions))\n",
    "\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy = mean_accuracy / iterations\n",
    "    log.debug(\"Decision Tree Classifier:\")\n",
    "    log.debug(\"Mean accuracy over \" + str(iterations) + \" iterations is: \" + str(mean_accuracy))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Make predictions using trained model.\n",
    "    log.debug(\"Prediction statistics using Decision Tree Classifier:\")\n",
    "    make_predictions(decision_tree_classifier_clf)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def multi_layer_perceptron_classifier_grid_search():\n",
    "    \"\"\"\n",
    "         Function performs a exhaustive grid search to find the best hyper-parameters for use training the model.\n",
    "\n",
    "         :return: Nothing.\n",
    "         \"\"\"\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    mlp_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MLPClassifier(activation='logistic', alpha=1e-1, batch_size='auto',\n",
    "                              beta_1=0.9, beta_2=0.999, early_stopping=True,\n",
    "                              epsilon=1e-08, hidden_layer_sizes=(5, 2),\n",
    "                              learning_rate='constant', learning_rate_init=1e-1,\n",
    "                              max_iter=1000, momentum=0.9, n_iter_no_change=10,\n",
    "                              nesterovs_momentum=True, power_t=0.5, random_state=1,\n",
    "                              shuffle=True, solver='sgd', tol=0.0001,\n",
    "                              validation_fraction=0.1, verbose=False, warm_start=False)),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1)],\n",
    "        # 'tfidf__use_idf': (True, False),\n",
    "        # 'clf__hidden_layer_sizes': [(15, 15), (50, 50)],\n",
    "        'clf__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "        'clf__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "        'clf__alpha': [1e-1, 1e-2, 1e-4, 1e-6, 1e-8],\n",
    "        # 'clf__batch_size': [5, 10, 20, 40, 80, 160],\n",
    "        'clf__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        'clf__learning_rate_init': [1e-1, 1e-3, 1e-5],\n",
    "        # 'clf__power_t': [0.1, 0.25, 0.5, 0.75, 1.0],\n",
    "        # 'clf__max_iter': [200, 400, 800, 1600],\n",
    "        # 'clf_shuffle': [True, False],\n",
    "        # 'clf__tol': [1e-1, 1e-2, 1e-4, 1e-6, 1e-8],\n",
    "        # 'clf__momentum': [0.1, 0.3, 0.6, 0.9],\n",
    "        # 'clf_nestesrovs_momentum': [True, False],\n",
    "        # 'clf_early_stopping': [True, False],\n",
    "        # 'clf__validation_fraction': [0.1, 0.2, 0.4],\n",
    "        # 'clf_beta_1': [0.1, 0.2, 0.4, 0.6, 0.8],\n",
    "        # 'clf_beta_2': [0.1, 0.2, 0.4, 0.6, 0.8],\n",
    "        # 'clf_epsilon': [1e-1, 1e-2, 1e-4, 1e-8],\n",
    "        # 'clf__n_iter_no_change': [1, 2, 4, 8, 16]\n",
    "\n",
    "    }\n",
    "\n",
    "    # Perform the grid search using all cores.\n",
    "    mlp_classifier_clf = GridSearchCV(mlp_classifier_clf, parameters, cv=5, iid=False,\n",
    "                                      n_jobs=-1)\n",
    "\n",
    "    # Train and predict on optimal parameters found by Grid Search.\n",
    "    mlp_classifier_clf.fit(tweet_train, target_train)\n",
    "    mlp_classifier_predictions = mlp_classifier_clf.predict(tweet_test)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(mlp_classifier_clf.cv_results_)\n",
    "        log.debug(\"The shape of the Multi Layer Perceptron Neural Network Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\n",
    "            \"The contents of the Multi Layer Perceptron Neural Network Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the Multi Layer Perceptron Neural Network Classifier is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, mlp_classifier_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Display the accuracy we obtained using the optimal parameters.\n",
    "    log.debug(\"Accuracy using Multi Layer Perceptron Neural Network Classifier Grid Search is: \")\n",
    "    log.debug(np.mean(mlp_classifier_predictions == target_test))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def multi_layer_perceptron_classifier():\n",
    "    \"\"\"\n",
    "    Function trains a Multi Layer Perceptron Neural Network Classifier.\n",
    "    \n",
    "    :return: none. \n",
    "    \"\"\"\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "    mlp_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', MLPClassifier(activation='identity', alpha=1e-1, batch_size='auto',\n",
    "                              beta_1=0.9, beta_2=0.999, early_stopping=True,\n",
    "                              epsilon=1e-08, hidden_layer_sizes=(5, 2),\n",
    "                              learning_rate='constant', learning_rate_init=1e-1,\n",
    "                              max_iter=1000, momentum=0.9, n_iter_no_change=10,\n",
    "                              nesterovs_momentum=True, power_t=0.5, random_state=1,\n",
    "                              shuffle=True, solver='lbfgs', tol=0.1,\n",
    "                              validation_fraction=0.1, verbose=False, warm_start=False)),\n",
    "    ])\n",
    "\n",
    "    # Predict n iterations and calculate mean accuracy.\n",
    "    mean_accuracy = 0.0\n",
    "    iterations = 1000\n",
    "    for index in range(0, iterations):\n",
    "        # Create randomized training and test set using our dataset.\n",
    "        create_training_and_test_set()\n",
    "\n",
    "        # from sklearn.preprocessing import StandardScaler\n",
    "        # scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "        # scaler.fit(tweet_train)\n",
    "        # tweet_train_scaled = scaler.transform(tweet_train)\n",
    "        # tweet_test_scaled = scaler.transform(tweet_test)\n",
    "\n",
    "        mlp_classifier_clf.fit(tweet_train, target_train)\n",
    "        mlp_classifier_predictions = mlp_classifier_clf.predict(tweet_test)\n",
    "\n",
    "        # Calculate the accuracy of our predictions.\n",
    "        accuracy = np.mean(mlp_classifier_predictions == target_test)\n",
    "\n",
    "        if debug_classifier_iterations:\n",
    "            # Measure accuracy.\n",
    "            log.debug(\"\\n\")\n",
    "            log.debug(\"Accuracy for test set predictions using Decision Tree Classifier:\")\n",
    "            log.debug(str(accuracy))\n",
    "            log.debug(\"\\n\")\n",
    "\n",
    "            log.debug(\"MLP_classifier Metrics\")\n",
    "            log.debug(metrics.classification_report(target_test, mlp_classifier_predictions,\n",
    "                                                    target_names=['economic', 'environmental', 'social']))\n",
    "\n",
    "            log.debug(\"MLP_classifier confusion matrix:\")\n",
    "            log.debug(metrics.confusion_matrix(target_test, mlp_classifier_predictions))\n",
    "\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy = mean_accuracy / iterations\n",
    "    log.debug(\"Multi Layer Perceptron Neural Network Classifier:\")\n",
    "    log.debug(\"Mean accuracy over \" + str(iterations) + \" iterations is: \" + str(mean_accuracy))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Make predictions using trained model.\n",
    "    log.debug(\"Prediction statistics using Multi Layer Perceptron Neural Network Classifier:\")\n",
    "    make_predictions(mlp_classifier_clf)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def logistic_regression_classifier_grid_search():\n",
    "    \"\"\"\n",
    "       Function performs a exhaustive grid search to find the best hyper-parameters for use training the model.\n",
    "\n",
    "       :return: Nothing.\n",
    "       \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Create randomized training and test set using our dataset.\n",
    "    create_training_and_test_set()\n",
    "\n",
    "    logistic_regression_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                                   multi_class='multinomial', n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    # What parameters do we search for?\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__tol': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n",
    "        'clf__C': [0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "        'clf__fit_intercept': [True, False],\n",
    "        'clf__class_weight': ['balanced', None],\n",
    "        'clf__solver': ['saga', 'newton-cg', 'sag', 'lbfgs'],\n",
    "        'clf__max_iter': [2000, 4000, 8000, 16000],\n",
    "        'clf__multi_class': ['ovr', 'multinomial'],\n",
    "    }\n",
    "\n",
    "    # Perform the grid search using all cores.\n",
    "    logistic_regression_classifier_clf = GridSearchCV(logistic_regression_classifier_clf, parameters, cv=5, iid=False,\n",
    "                                                      n_jobs=-1)\n",
    "\n",
    "    # Train and predict on optimal parameters found by Grid Search.\n",
    "    logistic_regression_classifier_clf.fit(tweet_train, target_train)\n",
    "    logistic_regression_classifier_predictions = logistic_regression_classifier_clf.predict(tweet_test)\n",
    "\n",
    "    if debug_pipeline:\n",
    "        # View all the information stored in the model after training it.\n",
    "        classifier_results = pd.DataFrame(logistic_regression_classifier_clf.cv_results_)\n",
    "        log.debug(\"The shape of the  Logistic Regression Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.shape)\n",
    "        log.debug(\n",
    "            \"The contents of the Logistic Regression Classifier model's result data structure is:\")\n",
    "        log.debug(classifier_results.head())\n",
    "\n",
    "    # Display the optimal parameters.\n",
    "    log.debug(\"The optimal parameters found for the Logistic Regression Classifier is:\")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        log.debug(\"%s: %r\" % (param_name, logistic_regression_classifier_clf.best_params_[param_name]))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Display the accuracy we obtained using the optimal parameters.\n",
    "    log.debug(\"Accuracy using Logistic Regression Classifier Grid Search is: \")\n",
    "    log.debug(np.mean(logistic_regression_classifier_predictions == target_test))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def logistic_regression_classifier():\n",
    "    \"\"\"\n",
    "    Function trains a Logistic Regression Classifier.\n",
    "    \n",
    "    :return: none. \n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    logistic_regression_classifier_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', LogisticRegression(C=1.0, class_weight=None, fit_intercept=False, max_iter=2000,\n",
    "                                   multi_class='ovr', penalty='l2', solver='sag', tol=1e-1)),\n",
    "    ])\n",
    "\n",
    "    # Predict n iterations and calculate mean accuracy.\n",
    "    mean_accuracy = 0.0\n",
    "    iterations = 1000\n",
    "    for index in range(0, iterations):\n",
    "\n",
    "        # Create randomized training and test set using our dataset.\n",
    "        create_training_and_test_set()\n",
    "\n",
    "        logistic_regression_classifier_clf.fit(tweet_train, target_train)\n",
    "        logistic_regression_classifier_predictions = logistic_regression_classifier_clf.predict(tweet_test)\n",
    "\n",
    "        # Calculate the accuracy of our predictions.\n",
    "        accuracy = np.mean(logistic_regression_classifier_predictions == target_test)\n",
    "\n",
    "        if debug_classifier_iterations:\n",
    "            # Measure accuracy.\n",
    "            log.debug(\"\\n\")\n",
    "            log.debug(\"Accuracy for test set predictions using Logistic Regression Classifier:\")\n",
    "            log.debug(str(accuracy))\n",
    "            log.debug(\"\\n\")\n",
    "\n",
    "            log.debug(\"LogisticRegression_classifier Metrics\")\n",
    "            log.debug(metrics.classification_report(target_test, logistic_regression_classifier_predictions,\n",
    "                                                    target_names=['economic', 'environmental', 'social']))\n",
    "\n",
    "            log.debug(\"LogisticRegression_classifier confusion matrix:\")\n",
    "            log.debug(metrics.confusion_matrix(target_test, logistic_regression_classifier_predictions))\n",
    "\n",
    "        mean_accuracy += accuracy\n",
    "\n",
    "    mean_accuracy = mean_accuracy / iterations\n",
    "    log.debug(\"Logistic Regression Classifier:\")\n",
    "    log.debug(\"Mean accuracy over \" + str(iterations) + \" iterations is: \" + str(mean_accuracy))\n",
    "    log.debug(\"\\n\")\n",
    "\n",
    "    # Make predictions using trained model.\n",
    "    log.debug(\"Prediction statistics using Logistic Regression Classifier:\")\n",
    "    make_predictions(logistic_regression_classifier_clf)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "def keras_deep_neural_network():\n",
    "    \"\"\"\n",
    "    Function implements a Keras Deep Neural Network Model.\n",
    "    TODO - most likely won't be implemented until summer research with much larger labeled datasets.\n",
    "    :return: none. \n",
    "    \"\"\"\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras import layers\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "############################################################################################\n",
    "\"\"\"\n",
    "Main function.  Execute the program.\n",
    "\"\"\"\n",
    "\n",
    "# Debug variable.\n",
    "debug_main = 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import time\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call non-pipelined multinomial Naive Bayes classifier training function.\n",
    "    # scikit_learn_multinomialnb_classifier_non_pipeline()\n",
    "\n",
    "    # Call pipelined classifier training functions and grid search functions.\n",
    "    # multinomial_naive_bayes_classifier_grid_search()\n",
    "    multinomial_naive_bayes_classifier()\n",
    "    # sgd_classifier_grid_search()\n",
    "    sgd_classifier()\n",
    "    # svm_support_vector_classification_grid_search()\n",
    "    svm_support_vector_classification()\n",
    "    # svm_linear_support_vector_classification_grid_search()\n",
    "    svm_linear_support_vector_classification()\n",
    "    # nearest_kneighbor_classifier_grid_search()\n",
    "    nearest_kneighbor_classifier()\n",
    "    # decision_tree_classifier_grid_search()\n",
    "    decision_tree_classifier()\n",
    "    # multi_layer_perceptron_classifier_grid_search()\n",
    "    multi_layer_perceptron_classifier()\n",
    "    # logistic_regression_classifier_grid_search()\n",
    "    logistic_regression_classifier()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    if debug_pipeline:\n",
    "        log.debug(\"The time taken to train the classifier(s) is:\")\n",
    "        total_time = end_time - start_time\n",
    "        log.debug(str(total_time))\n",
    "        log.debug(\"\\n\")\n",
    "\n",
    "    # For debug purposes.\n",
    "    # my_set = create_prediction_set()\n",
    "\n",
    "############################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Final Project Proposal - Draft 2 - Preliminary Draft of Final Paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Social License to Operator Triple-Bottom-Line Tweet Classification</p>\n",
    "\n",
    "\n",
    "The application domain is the Triple-Bottom-Line (TBL) classification of Tweet in the context of the Social License to Operate (SLO) of mining companies.  The objective of this project is to continue and extend the earlier work on Tweet TBL classification done at CSIRO – the Commonwealth Scientific and Industrial Research Organization (Australia’s National Science Agency).  The goal is to set up a prototype machine learning system that is capable of identifying the topic classification of a Tweet as either Environmental, Social, or Economic.  The initial milestone is to achieve at an absolute minimum a 50% accuracy metric or higher, indicating the ability to at least guess on par with a flip of a coin.</p>\n",
    "\t\n",
    "    \n",
    "The Social License to Operate is defined as when an existing project has the ongoing approval of the local community and other stakeholders within the domain the project operates in.  It is the ongoing social acceptance of that project in regards to a favorable or dis-favorable disposition by those who are concerned.  The SLO must not only be earned but also maintained as the beliefs, opinions, and perceptions of people tend to be dynamic over the course of time.  It is beneficial to the project owners and managers to maintain an agreeable relationship with the local population and their stakeholders.</p>\n",
    "\t\n",
    "    \n",
    "The Triple Bottom Line is defined as a framework where organizations and companies dedicate themselves not only to profit but also the social and environmental impact of their operation.  The phrase was coined by the British management consultant John Elkington as a metric to measure the performance of corporate America.  According to Investopedia, the corporate business should be done according to:</p>\n",
    "\n",
    "\n",
    "Profit – the traditional measure of corporate profit – the profit and loss (P & L) account.</p>\n",
    "\n",
    "People – the measure of how socially responsible an organization has been throughout its operations.</p>\n",
    "\n",
    "Planet – the measure of how environmentally responsible a firm has been.</p>\n",
    "\n",
    "These are the three elements of TBL which are then sourced into the terms Economy (profit), Environmental (planet), and Social (people).</p>\n",
    "\t\n",
    "    \n",
    "Twitter data (Tweets) can be obtained in 4 distinct ways – retrieval from the Twitter public API, use of an existing Twitter dataset, purchase from Twitter directly, or purchase access from a 3rd party Twitter service provider.  For the purposes of this project, we will be using existing Twitter datasets provided by Professor VanderLinden via access to Calvin College’s Borg supercomputer.  Specifically, we will be using a training set consisting of crowdsourced Triple Bottom Line labeled Tweets used by CSIRO in their preliminary topic classification research.  For the test set, we will be using a small dataset consisting of TBL labeled Tweets hand-labeled by Professor VanderLinden.  With the machine learning model trained on these two sets, we will then generalize the model to make predictions on the dataset used for stance classification of Tweets in earlier research by Professor VanderLinden and Roy Adams.</p>\n",
    "\t\n",
    "    \n",
    "As our research is a continuation of prior research from CSIRO and based on the foundation laid by Professor VanderLinden’s “Machine Learning for Social Media” project, we see no reason to not use machine learning.  While we might consider symbolic artificial intelligence (GOFAI – Good, Old-Fashioned AI), we learned in CS-344 that symbolic reasoning implementations resulted in rules engines, also known as expert systems or knowledge graphs.  These proved to be too brittle and became unmanageable as the knowledge base grew beyond a few thousand rules.  Considering the nature of Tweets, GOFAI seems not to be a viable solution.  The language of Tweets is often informal, prone to slang, misuse of established grammatical rules, and in general a chimeric bastardization of known human languages (insofar in my experience).  It is doubtful a purely symbolic AI would be computationally feasible.  Perhaps as Professor VanderLinden mentioned, a hybrid A.I. combining symbolic reasoning and deep neural networks is the future of A.I. and would prove to be a feasible approach.</p>\n",
    "\n",
    "\n",
    "Preliminary analysis of the two provided datasets indicates that they will require significant pre-processing before becoming useable as input features for machine learning.  The Tweets are stored as comma delimited CSV files.  The training dataset consists of 299 total Tweets, of which 198 are unlabeled due to not being associated with any TBL classification.  The test dataset consists of 31 hand-labeled Tweets.  Based on the size of the datasets we are working with neural networks may not be the best choice to start with.  Neural networks typically require larger datasets in order to train and as we barely have 330 total examples to work with, the results may be less than optimal.  Therefore, we will start with Bayesian models and SVM’s – Support Vector Machines.  Later, we will expand to using supervised neural networks just to see if we can tune hyperparameters to obtain results closely comparable to our non-NN models.</p>\n",
    "\t \n",
    "     \n",
    "For fast prototyping, we will be using Scikit-Learn in Python rather than Keras or straight Tensorflow, at least until we have established which baseline supervised learning algorithm will provide us with the potential for the best results.  We will also use Pandas, built on NumPy, for data-frame manipulation and matplotlib for visualizations.  To encode our categorical Tweet data into useable numerical Tweet data, we will be using the tools provided by Scikit-Learn.</p>\n",
    "\t\n",
    "    \n",
    "Our Bayesian model will be the MultinomialNB classifier that implements the naïve Bayes algorithm for multinomially distributed data.  Scikit-Learn.org indicates that it is one of the two classic Naïve Bayes variants used in textual classification problems.  This indicates it will be an excellent starting point as we have decided our two datasets are too small to initially warrant the use of a supervised neural network training algorithm.  “Naïve” in this case indicates the application of Bayes’ theorem with the “naïve” assumption of conditional independence between every pair of features given the value of the class variable (4).  Further information indicates the classifier performs fast and works in many real-world applications, including document classification and spam filtering.  We built a spam filter based on Paul Graham’s “A Plan for Spam” and indeed it worked well.</p> \n",
    "\t\n",
    "    \n",
    "Our SVM classification model will be the LinearSVC Classifier– Linear Support Vector Classification.  Sci-Kit Learn indicates it is effective in high dimensional spaces and when the number of dimensions is greater than the number of samples.  This will be the case for us as we have a limited 330 samples and after multi-hot encoding to form a feature vector to create a bag-of-words vocabulary, our dimensionality is bound to be pretty high in comparison to the samples.  The memory efficiency of this algorithm should also help as we will no doubt have sparse vectors in comparison to the total vocabulary present across all of the Tweets.  Of note, is that SVM algorithms are not scaling invariant, so data scaling is required, which will matter in our case as encoding our categorical word data will result in word occurrence values for the input feature vector (unless we choose to simply represent as binary: 0 – word not present and 1- word is present). API documentation indicates that the classifier supports sparse input (good for us) and supports multi-class using the one-vs-the-rest scheme.</p>\n",
    "\t\n",
    "    \n",
    "Our deep neural network will be the MLP Classifier – multi-layer perceptron.  Scikit-Learn indicates it uses a Softmax layer as the output function to perform multi-class classification and uses the cross-entropy loss function.  MLP also supports multi-label classification through use of the logistic activation function where values > 0.5  1 and values < 0.5  0.  Given this, it would be possible for us to perform multi-class multi-label TBL classification on our training dataset.  Our training dataset does possess Tweets that have been given multiple topic classifications, although some are redundant duplicates of either economic, social, or environmental.  We will leave this possibility for the future, time permitting.  Effective use of the MLP classifier would most likely require us to hand-label additional training example from the larger Twitter datasets present on Calvin’s Borg supercomputer.  Crowdsourcing does not seem a viable option so this task would be tediously time-consuming.</p>\n",
    "\t\n",
    "    \n",
    "The application of machine learning to Social License to Operate on Triple-Bottom-Line topic classification can potentially assist any organization or company in evaluating their current level of acceptability by the local population and relevant stakeholders.  Specifically, it could help evaluate whether people are more concerned about the economic, social, or environmental aspects of the project.  In conjunction with stance and sentiment SLO machine learning models, it should be plausible that the level of acceptability of a project can be accurately judged.</p>\n",
    "\t\n",
    "    \n",
    "With social media so prevalent in this day and age, it is a simple matter to obtain fresh new datasets on a daily basis to gauge the SLO.  As such, the synchronicity between the dynamism of maintaining the SLO and new Tweets pertaining to the associated project works well.  Rather than conduct old fashioned mail surveys, which is time-consuming and potentially expensive, the entire procedure can be automated.  Extract Twitter data using the Twitter API, pre-process the dataset, post-process the dataset, insert into the machine learning model(s) as input feature vectors, and predict the level of approval.  Given a good model, any organization, corporation, or other entity, can perform a pseudo-real-time estimate on how accepted their current operations and activities are.</p>\n",
    "\t\n",
    "    \n",
    "The initial investment would be in adjusting hyperparameters with the validation set to achieve the optimal results while avoiding overfitting and ensuring the model generalizes well to new data.  Once this is achieved, the model should be relevant and usable as an SLO predictor for a given period of time for a particular project and organization.  Of course, even with a good model perhaps the best way to judge SLO would still be to do a face-to-face interview with the individuals in the community and stakeholders and simply ask how they feel about the project.  Then again, the anonymity of the Internet does provide an outlet for people to vent and voice their opinions with less fear of reprisal than in reality.  So perhaps anonymous Tweeters are more honest.  But, anonymity could also cause people to simply say whatever they desire with little regard to how their words actually correlate to their own personal beliefs and opinions on the matter.  Either way, an SLO TBL machine learned prediction model won’t be the be all and end all in estimating Social License to Operate.  But, it can be a useful cog in the whole machine in order to generate the necessary analysis required to measure the components of SLO.</p>\n",
    " \n",
    " \n",
    "Works Referenced:\n",
    "\n",
    "\n",
    "1)\tAnonymous ACL submission. “Classifying Stance Using Profile Texts”.</p>\n",
    "\n",
    "2)\t“1. Supervised Learning¶.” Scikit, scikit-learn.org/stable/supervised_learning.html#supervised-learning.</p>\n",
    "\n",
    "3)\t“A Gentle Introduction to the Bag-of-Words Model.” Machine Learning Mastery, 12 Mar. 2019, machinelearningmastery.com/gentle-introduction-bag-words-model/.</p>\n",
    "\n",
    "4)\t“Introduction to Machine Learning  |  Machine Learning Crash Course  |  Google Developers.” Google, Google, developers.google.com/machine-learning/crash-course/ml-intro.</p>\n",
    "\n",
    "5)\tKenton, Will. “How Can There Be Three Bottom Lines?” Investopedia, Investopedia, 9 Apr. 2019, www.investopedia.com/terms/t/triple-bottom-line.asp.</p>\n",
    "\n",
    "6)\tLittman, Justin. “Where to Get Twitter Data for Academic Research.” Social Feed Manager, 14 Sept. 2017, gwu-libraries.github.io/sfm-ui/posts/2017-09-14-twitter-data.</p>\n",
    "\n",
    "7)\tMohammad, Saif, et al. “SemEval-2016 Task 6: Detecting Stance in Tweets.” Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), 2016, doi:10.18653/v1/s16-1003.</p>\n",
    "\n",
    "8)\t“Multiclass Classification.” Wikipedia, Wikimedia Foundation, 18 Apr. 2019, en.wikipedia.org/wiki/Multiclass_classification.</p>\n",
    "\n",
    "9)\t“Symbolic Reasoning (Symbolic AI) and Machine Learning.” Skymind, skymind.ai/wiki/symbolic-reasoning.\n",
    "10)\tWalker, Leslie. “Learn Tweeting Slang: A Twitter Dictionary.” Lifewire, Lifewire, 8 Nov. 2017, www.lifewire.com/twitter-slang-and-key-terms-explained-2655399.</p>\n",
    "\n",
    "11)\t“What Is the Social License?” The Social License To Operate, socialicense.com/definition.html.</p>\n",
    "\n",
    "12)\t“Working With Text Data¶.” Scikit, scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Social License to Operate: Triple-Bottom-Line Topic Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Vision:</p>\n",
    "\n",
    "\n",
    "The general purpose of the project is to perform Social License to Operate Triple-Bottom-Line topic classification on Twitter data associated with various mining companies.  Social License to Operate indicates the ongoing acceptance of a company or industry’s standard business practices and operating procedures by its employees, stakeholders, and the general public (Investopedia).  Triple Bottom Line is a framework or theory that recommends that companies commit to focus on social and environmental concerns just as they do on profits (Investopedia).  We will use supervised machine learning algorithms to perform multi-class single-label classification Tweets to predict whether their topic of discussion corresponds to social, environmental, or economic concerns.</p>\n",
    "\n",
    "\n",
    "Background:</p>\n",
    "\n",
    "\n",
    "Our work is a revival and continuation of the work initially done at the Commonwealth Scientific and Industrial Research Organization (CSIRO) by (insert name here) on TBL topic classification.  We are not directly referencing that research but instead basing our initial data pre-processing on the anonymous ACL submission titled “Classifying Stance Using Profile Text”.  We are however using the exact same labeled training dataset that was used in the prior research for TBL topic classification on SLO for mining companies.  Our work will also involve use of the datasets available on Calvin College’s Borg Supercomputer and will be uploaded to the Calvin-CS / slo-classifiers GitHub Repository.  This project will be a prelude to continued research on topic, stance, and sentiment analysis utilizing machine learning for Social License to Operate of mining companies in connection with Professor VanderLinden’s “Machine Learning for Social Media” research project.</p>\n",
    "\n",
    "\n",
    "As of the current status of this report, we are currently rapid prototyping using Scikit-Learn machine learning classifiers.  These classifiers require minimal effort to initially setup with default hyperparameters.  They train speedily and provide results in a timely manner, allowing us to adjust our hyper-parameters on-the-fly to see if there are any noticeable differences.  It is also quite simple to add additional Classifiers as the Pipeline class allows literal copy/paste of a code template.  All that is required is the addition of a new import statement for that Classifier and to replace the name of the old Classifier and its corresponding parameters with the new one.  This design feature is one of the reasons we chose to utilize Scikit-Learn; that and it was recommended by Professor VanderLinden as the starting point.</p>  \n",
    "\n",
    "\n",
    "Of note is that Scikit-Learn provides automated parameter tuning via the Grid Search and Random Search classes.  Grid search methodically builds and evaluates a model for each combination of algorithm parameters specified in a grid.  Random search methodically builds and evaluates a model for each combination of algorithm parameters sampled from a random distribution for a fixed number of iterations.  We plan to utilize one or both of these parameter tuning methods in order to expedite the search for optimal hyperparameters for all of the Scikit-Learn Classifiers we are prototyping with.  As we add additional Classifiers to our codebase, it becomes time-saving to automate parameter tuning as much as possible.</p>\n",
    "\n",
    "\n",
    "Once we have established which classifiers have the most potential to provide favorable metrics, we may migrate towards Keras and Tensorflow for GPU support and more versatility.  Scikit-Learn does not provide GPU support for its machine learning algorithms.  This does not matter at the moment as we are working with two very small datasets which in total only provide us with 330 samples.  That and GPU support will primarily benefit deep neural networks while we are also using non-NN algorithms.  However, if we wish to crowdsource TBL classification on significantly larger Twitter datasets and work with those, then GPU support will become necessary.  We have heard it requires approximately 24 hours utilizing one Nvidia Geforce Titan on the Borg supercomputer to perform stance analysis training on the larger Twitter datasets consisting of 500k+ examples.  It would be expedient to parallelize this process utilizing all 4 Nvidia Geforce Titans on Borg to cut the training time down to a quarter.</p>\n",
    "\n",
    "\n",
    "We plan to implement metric visualizations via the use of the matplotlib library and SciView in Pycharm.  The Scikit-learn online documentation has a section on “Classification of text documents using sparse features” that can hopefully be modified to suit our purposes.  Their codebase constructs a bar plot comparing a variety of Classifiers side-by-side visualizing the accuracy score, training time, and test time.  As we are also training multiple Classifiers in the hopes of finding a suitable one(s) to further explore in the Keras and Tensorflow API, this type of visualization would be very useful.  Individual charts detailing a metric summarization of the micro/macro average, weighted average and associated precision, recall, f1-score, and support values are also planned.</p>\n",
    "\n",
    "\n",
    "Implementation:</p>\n",
    "\n",
    "\n",
    "These sections will describe in detail (perhaps too much detail) our current implementation for SLO TBL topic classification in Python in association with the current state of the codebase.  We have decided to keep all debug output statements in the meantime as this is far from the final system that will be implemented.</p>\n",
    "\n",
    "\n",
    "We are performing text pre-processing on the training dataset that consists of 229 Tweet examples.  Not all of them are labeled with a TBL topic classification and those are dropped from consideration.  The data is shuffled randomly upon importation to ensure there is no biased structure to the import order.  We do so by utilizing Numpy’s “random. permutation” function.  Then, a Pandas dataframe is constructed to store the dataset.  Custom column names are added for clarity of purpose as none originally exist.  The “Tweet” column stores the Tweet, “SLO1” stores the first assigned topic label, “SLO2” and “SLO3” do the same.</p>\n",
    "\n",
    "\n",
    "Pandas provide a “dropna()” method by which we drop all rows without at least 2 non-NaN values.  This indicates that the example lacks any TBL classification labels and can be safely discarded.  We use Boolean indexing via bitwise operations, the “.notna()” method, to construct a mask by which we isolate those examples with only a single TBL classification.  These examples are placed in a new dataframe and afterward, we drop the SLO2 and SLO3 columns as they are obviously just NaN values.  This procedure is effective as a preliminary analysis of the CSV file indicates that all labeled examples definitely have a label in the “SLO1” column.  Our objective is to construct a dataframe consisting of a column storing the raw Tweet and another column storing a single topic classification.  We rename this new dataframe to columns “Tweet” and “SLO”.</p>\n",
    "\n",
    "\n",
    "Next, we construct another mask to isolate all examples with multiple SLO TBL classifications and apply the mask to construct a new dataframe containing only those examples.  We then perform a “drop()” operation on the new dataframe to construct 3 separate dataframes.  The first from dropping SLO2 and SLO3, second dropping SLO1 and SLO3, and third dropping SLO1 and SLO2.  This inefficient but workable solution effectively create duplicates of all examples with multiple SLO TBL classifications with just a single label per example.  We then name the columns “Tweet” and “SLO”.  This is done so that our machine learning model can take into consideration those examples that can be classified as multiple topics.</p>\n",
    "\n",
    "\n",
    "The multiple separate dataframes constructed from the above operations are then concatenated back together as a single whole Pandas dataframe.  Any rows with a NaN value in any column are then dropped via “dropna()” to effectively remove all examples with multiple topic classifications that might have had a topic in SLO2 but not SLO3 or vice versa.  Last, we drop all duplicated examples possessing the same TBL classification values in the “SLO” column.  We do this as the initial imported dataset sometimes contained duplicate labels for the same example.  We surmise this is because multiple people were manually hand-tagging the Tweets and sometimes they were in agreement.</p>\n",
    "\n",
    "\n",
    "Using the “shape()” method call, our final training dataframe contains a total of 245 Tweets with a single TBL topic classification label.  It should be noted that as of our current implementation the second TBL labeled dataset provided by Professor VanderLinden is not currently in use.  There are 31 additional Tweets and we plan to include these in the future to help alleviate our issue of a small training and test dataset.  We are also using a large Twitter dataset that has already been pre-processed and tokenized as the set we will make predictions on in order to test the generalization of our model(s) to new data.  This set does not contain any target labels and thus we cannot use part of it to supplement our small training and test sets.  There are a total of 658983 Tweets included.  The CMU Tweet Tagger was used to pre-process the text but unfortunately, this is not a feasible option for us as we are working solely on Windows OS workstation(s).</p>\n",
    "\n",
    "\n",
    "As we are incapable of using the Linux/Mac only CMU Tweet Tagger for pre-processing, our decision was to manually clean the raw Tweet using Python regular expressions and other libraries.  The Natural Language Toolkit was considered as an alternative but ultimately we chose to just use built-in Python libraries and functions.  A for loop is used to send each Tweet to a preprocessing function that does the following:</p>\n",
    "\n",
    "\n",
    "a)\tRemoves “RT” tags indicating retweets.</p>\n",
    "\n",
    "b)\tRemoves URL. (e.ge. https//…) and replace with slo_url.</p>\n",
    "\n",
    "c)\tRemoves Tweet mentions (e.g. @mention) and replaces with slo_mention.</p>\n",
    "\n",
    "d)\tRemoves Tweet hashtags (e.g. #hashtag) and replaces with slo_hashtag.</p>\n",
    "\n",
    "e)\tRemoves all punctuation from the Tweet.</p>\n",
    "\n",
    "\n",
    "We also down-case all text from upper to lower case letters.  On our TODO list is to implement regular expressions or other methods in order to:</p>\n",
    "\n",
    "\n",
    "a)\tShrink character elongations (e.g. “yeees”  “yes”)</p>\n",
    "\n",
    "b)\tRemove non-English tweets</p>\n",
    "\n",
    "c)\tRemove non-company associated Tweets.</p>\n",
    "\n",
    "d)\tRemove year and time.</p>\n",
    "\n",
    "\n",
    "For our current two datasets, the yet-to-be-implemented preprocessing features do not seem to be an issue as the preliminary analysis indicates those elements are not present or have already been considered.</p>\n",
    "\n",
    "\n",
    "The next step was the input feature creation using the “Tweet” column and a target label set using the “SLO” column.  Scikit-Learn included a handy function “train_test_split()” which allowed us to easily split our input feature and target labels into a training and test set.  The target label train and test sets were then encoded using the Scikit-Learn LabelEncoder class.  This converted our categorical labels of “economic”, “environmental”, and “social”, into associated integer values of 0, 1, and 2, respectively.  A necessary step as most machine learning algorithms we are interested in prototyping with require and support only numerical data.</p>\n",
    "\n",
    "\n",
    "The Scikit-Learn CountVectorizer class was used to convert the processed Tweet training and test set into feature vectors with binary values of 0 and 1.  Documentation indicates that the class converts a collection of text documents to a matrix of token counts and produces a sparse representation of the counts.  As we did not provide an a-priori dictionary and analyzer for feature selection, the total number of features is equal to the vocabulary size of the analyzed data.  Hence, we have a very high dimensionality in our feature vectors compared to our small number of samples.  This effectively creates the bag-of-words that we used to represent our categorical Tweet data.  The occurrences of each word are stored in the feature vector.  Console output shows that we are dealing with a vocabulary size of 809 in comparison to 164 examples for the training set and 81 examples for the test set.</p>\n",
    "\n",
    "\n",
    "The Scikit-Learn TfidfTransformer class was then used to convert the vectorized categorical Tweet data into term-frequency * inverse document-frequency.  The purpose of this is to scale down the impact of tokens that occur very frequently and are therefore empirically less informative than features that occur in a small fraction of the training set.  Term frequencies, in general, are better than raw occurrences as larger corpuses will have higher average word occurrence values than smaller corpuses.  So, normalization of this kind provides better input feature vectors for training our model.</p>\n",
    "\n",
    "\n",
    "It is at this point in the code base that we also import a very large Tweet dataset consisting of some 600k+ Tweets that are unlabeled to be used as the input feature for making predictions and seeing how well our model generalizes to new data.  These Tweets have already been preprocessed and tokenized by the CMU Tweet Tagger.  We simply have to run the entire dataset into a Pandas dataframe, isolate the “tweet_t” column that contains the Tweet, and use the CountVectorizer and TfidfTransformer class to normalize from categorical to numerical data.  The details are similar to what is described above.  For the future, we plan to do further post-processing on these Tweets in order to minimize the discrepancies between how we pre-processed and post-processed our training and test datasets and how it was done on this Twitter dataset.  Two things we have noticed is that those Tweets still seem to contain hashtag items and some punctuation.  These should be removed as we removed both in our training and test sets.  The predictive ability of our trained model may otherwise be compromised when using these Tweets.\n",
    "With the training, test, and generalization set properly prepared, we utilized Scikit-Learn’s Pipeline class in order to set up various Classifiers.  These currently include:</p>\n",
    "\n",
    "\n",
    "a)\tMultinomial Naïve Bayes’</p>\n",
    "\n",
    "b)\tStochastic Gradient Descent (SGD)</p>\n",
    "\n",
    "c)\tSupport Vector Machine – Support Vector Classifier.</p>\n",
    "\n",
    "d)\tSupport Vector Machine – Linear Support Vector Classifier.</p>\n",
    "\n",
    "e)\tNearest Neighbor KNeighbors Classifier.</p>\n",
    "\n",
    "f)\tDecision Tree Classifier.</p>\n",
    "\n",
    "g)\tMulti-layer Perceptron Neural Network Classifier.</p>\n",
    "\n",
    "h)\tLogistic Regression Classifier.</p>\n",
    "\n",
    "\n",
    "These are all Classifiers capable of multi-class single-label topic classification.  As such, we have decided to implement as many as we can to see which one will be the most performant and worthy of further consideration in the Keras and Tensorflow API, provided those API’s support or can be made to support that Classifier.</p>\n",
    "\n",
    "\n",
    "Results:</p>\n",
    "\n",
    "\n",
    "As we have just begun initial implementation of our machine learning system, most of these classifiers have been using default hyperparameters and thus our results have been pretty dismal, at best.  The highest accuracy metric obtained was almost 56% with the lowest dipping in the 20th percentile.  It is our plan to use parameter tuning via Grid Search or Random Search to assist in finding hyperparameters that will improve our metrics.  As of the moment, the predictive ability of our Scikit-Learn trained models is less useful than flipping a coin.</p>\n",
    "\n",
    "\n",
    "Of particular concern to us is performing the proper and necessary pre-processing and post-processing of the Twitter data into useable sparse feature vectors.  Regretfully, we will need to obtain the assistance of other researchers with a Linux/Mac workstation and the proper set up in order to use the CMU Tweet Tagger on the labeled TBL datasets.  Otherwise, we can only find other alternatives.</p>\n",
    "\n",
    "\n",
    "It is also within our planned schedule to implement matplotlib visualizations of our metric summaries to display the results of training our models and their predictive abilities in generalizing to new data.  As of the current writing of this report, this is where are at in our research efforts.  Please refer to the code modules included in this Jupyter Notebook for further details.</p>\n",
    "\n",
    "\n",
    "Placeholder – discuss comparison with similar works.</p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "Works Referenced:</p>\n",
    "\n",
    "\n",
    "1)\t“1. Supervised Learning¶.” Scikit, scikit-learn.org/stable/supervised_learning.html#supervised-learning.</p>\n",
    "\n",
    "2)\t“A Gentle Introduction to the Bag-of-Words Model.” Machine Learning Mastery, 12 Mar. 2019, machinelearningmastery.com/gentle-introduction-bag-words-model/.</p>\n",
    "\n",
    "3)\t“Classification of Text Documents Using Sparse Features¶.” Scikit, scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py.</p>\n",
    "\n",
    "4)\t“Introduction to Machine Learning  |  Machine Learning Crash Course  |  Google Developers.” Google, Google, developers.google.com/machine-learning/crash-course/ml-intro.</p>\n",
    "\n",
    "5)\t“How to Tune Algorithm Parameters with Scikit-Learn.” Machine Learning Mastery, 1 Nov. 2018, machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/.</p>\n",
    "\n",
    "6)\tKenton, Will. “How Can There Be Three Bottom Lines?” Investopedia, Investopedia, 9 Apr. 2019, www.investopedia.com/terms/t/triple-bottom-line.asp.</p>\n",
    "\n",
    "7)\tLittman, Justin. “Where to Get Twitter Data for Academic Research.” Social Feed Manager, 14 Sept. 2017, gwu-libraries.github.io/sfm-ui/posts/2017-09-14-twitter-data.</p>\n",
    "\n",
    "8)\tMohammad, Saif, et al. “SemEval-2016 Task 6: Detecting Stance in Tweets.” Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), 2016, doi:10.18653/v1/s16-1003.</p>\n",
    "\n",
    "9)\t“Multiclass Classification.” Wikipedia, Wikimedia Foundation, 18 Apr. 2019, en.wikipedia.org/wiki/Multiclass_classification.</p>\n",
    "\n",
    "10)\t“Symbolic Reasoning (Symbolic AI) and Machine Learning.” Skymind, skymind.ai/wiki/symbolic-reasoning.</p>\n",
    "\n",
    "11)\tWalker, Leslie. “Learn Tweeting Slang: A Twitter Dictionary.” Lifewire, Lifewire, 8 Nov. 2017, www.lifewire.com/twitter-slang-and-key-terms-explained-2655399.</p>\n",
    "\n",
    "12)\t“What Is the Social License?” The Social License To Operate, socialicense.com/definition.html.</p>\n",
    "\n",
    "13)\t“Working With Text Data¶.” Scikit, scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
